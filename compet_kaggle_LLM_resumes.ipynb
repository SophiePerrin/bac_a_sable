{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "590d7930119a40ec80852b61f1d745f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7494184df6a4d76be440b3aa8e450ed",
              "IPY_MODEL_20f371d49459494dab86ffabc022bd8b",
              "IPY_MODEL_6f42d7009347405b9f171086ff5d006f"
            ],
            "layout": "IPY_MODEL_e15000d777124cf1a0d716f9a5dfaf94"
          }
        },
        "c7494184df6a4d76be440b3aa8e450ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_087cef66d27d488aaaaed55dd832b0e8",
            "placeholder": "​",
            "style": "IPY_MODEL_8c661e2b93de4efeaef3280a2fec9e24",
            "value": "Map: 100%"
          }
        },
        "20f371d49459494dab86ffabc022bd8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c18877d0dc04869878441ec29cebda9",
            "max": 402,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1895bff15c9a4779a1e15f01e88dd63d",
            "value": 402
          }
        },
        "6f42d7009347405b9f171086ff5d006f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f47d699049e94e018ef36107abf72b43",
            "placeholder": "​",
            "style": "IPY_MODEL_4eb60c52894a421ca2644f12b86a1a62",
            "value": " 402/402 [00:00&lt;00:00, 432.53 examples/s]"
          }
        },
        "e15000d777124cf1a0d716f9a5dfaf94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "087cef66d27d488aaaaed55dd832b0e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c661e2b93de4efeaef3280a2fec9e24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c18877d0dc04869878441ec29cebda9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1895bff15c9a4779a1e15f01e88dd63d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f47d699049e94e018ef36107abf72b43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eb60c52894a421ca2644f12b86a1a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SophiePerrin/bac_a_sable/blob/main/compet_kaggle_LLM_resumes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confection de résumés automatiques d'articles scientifiques par un modèle de langue pré-entraîné : étude et comparaison des performances de deux modèles.\n",
        "\n",
        "**Auteur.e.s :** Sophie perrin, Emma, El Zube, Marius, Sylvano, pour le cours *Representation learning for NLP* @ Master 2 MALIA et MIASHS."
      ],
      "metadata": {
        "id": "Jm3aCEp_w2w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Préparation de l'environnement\n",
        "\n",
        "1. Se connecter ou se créer un compte sur https://huggingface.co/\n",
        "2. Se créer un nouveau token d'accès : https://huggingface.co/settings/tokens\n",
        "3. Enregistrer ce token comme un secret nommé `HF_TOKEN` dans Google Colab (icone \"clef\" dans le bandeau vertical)\n",
        "4. Exécuter le code ci-dessous\n",
        "\n",
        "Note importante pour la vitesse de calcul : pour activer l'accélération GPU dans votre notebook Colab :\n",
        "\n",
        "    Ouvrez le menu « Exécution » : Dans votre notebook Colab, cliquez sur le menu « Exécution » en haut de l'écran.\n",
        "\n",
        "    Sélectionnez « Modifier le type d'exécution » : Dans le menu déroulant, choisissez « Modifier le type d'exécution ».\n",
        "\n",
        "    Choisissez le matériel GPU : Une fenêtre contextuelle s'affiche. Dans la section « Accélérateur matériel », sélectionnez « GPU » dans le menu déroulant.\n",
        "\n",
        "    Enregistrez les modifications : Cliquez sur « Enregistrer » pour appliquer les modifications.\n",
        "\n"
      ],
      "metadata": {
        "id": "oiCaaZ8YC8hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pp0itMoj78l",
        "outputId": "6fc6780e-9cd4-447f-c06d-f7f325e10549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login; login(token=userdata.get(\"HF_TOKEN\"))\n",
        "\n",
        "from nltk import download; download('punkt_tab')\n",
        "!pip install -U bitsandbytes txtai\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z-S3zGw4BfUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b91c7e68-5d81-41a5-e693-63e126680a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.0)\n",
            "Requirement already satisfied: txtai in /usr/local/lib/python3.10/dist-packages (8.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.13.1+cu117)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: faiss-cpu>=1.7.1.post2 in /usr/local/lib/python3.10/dist-packages (from txtai) (1.9.0.post1)\n",
            "Requirement already satisfied: msgpack>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from txtai) (1.1.0)\n",
            "Requirement already satisfied: transformers>=4.45.0 in /usr/local/lib/python3.10/dist-packages (from txtai) (4.47.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from txtai) (0.27.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from txtai) (6.0.2)\n",
            "Requirement already satisfied: regex>=2022.8.17 in /usr/local/lib/python3.10/dist-packages (from txtai) (2024.11.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu>=1.7.1.post2->txtai) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->txtai) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->txtai) (2024.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->txtai) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->txtai) (4.67.1)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.45.0->txtai) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.45.0->txtai) (0.4.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->txtai) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->txtai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->txtai) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->txtai) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Préparation des données d'entraînement pour leur utilisation par le modèle"
      ],
      "metadata": {
        "id": "hoSW39T9djGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée un dictionnaire qui apparie les articles et leurs résumés par leur identifiant commun."
      ],
      "metadata": {
        "id": "iSOUl2N1dtAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Chemins des dossiers contenant les fichiers\n",
        "dossier_abstracts = \"/content/drive/MyDrive/donnees/OBS/abstracts_OBS/\"\n",
        "dossier_articles = \"/content/drive/MyDrive/donnees/OBS/articles_OBS/\"\n",
        "\n",
        "# Liste des fichiers dans chaque dossier\n",
        "fichiers_abstracts = [f for f in os.listdir(dossier_abstracts) if f.startswith(\"abstract-\")]\n",
        "fichiers_articles = [f for f in os.listdir(dossier_articles) if f.startswith(\"article-\")]\n",
        "\n",
        "# Dictionnaires pour stocker les fichiers par identifiant\n",
        "abstracts = {}\n",
        "articles = {}\n",
        "\n",
        "# Remplir les dictionnaires avec les fichiers en fonction des identifiants\n",
        "for fichier in fichiers_abstracts:\n",
        "    # Extraire l'identifiant du fichier abstract\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "     #fichier.split(\"-\") : La méthode split(\"-\") divise le nom du fichier\n",
        "    #en une liste de sous-chaînes en utilisant le caractère \"-\" comme séparateur.\n",
        "    #Par exemple, si le fichier est \"abstract-123.txt\", fichier.split(\"-\") renverra la liste [\"abstract\", \"123.txt\"].\n",
        "    #fichier.split(\"-\")[1] : En prenant l'élément d'indice 1 de cette liste (c'est-à-dire \"123.txt\"), on obtient la partie du nom du fichier après le préfixe \"abstract-\".\n",
        "    #split(\".\")[0] : Ensuite, on divise cette chaîne \"123.txt\" avec split(\".\"), ce qui donne [\"123\", \"txt\"].\n",
        "    #On prend le premier élément de la liste (c'est-à-dire \"123\") qui est l'identifiant unique de l'abstract.\n",
        "    abstracts[identifiant] = fichier\n",
        "    #Cette ligne ajoute une entrée dans le dictionnaire abstracts, où la clé est : identifiant (par exemple \"123\")\n",
        "    #et la valeur est le nom du fichier : fichier (par exemple \"abstract-123.txt\").\n",
        "\n",
        "for fichier in fichiers_articles:\n",
        "    # Extraire l'identifiant du fichier article\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "    articles[identifiant] = fichier\n",
        "\n",
        "# Dictionnaire pour stocker les appariements\n",
        "appariements = {}\n",
        "\n",
        "# Liste des abstracts et articles non-appariés\n",
        "non_apparies_abstracts = []\n",
        "non_apparies_articles = []\n",
        "\n",
        "# Appariement des fichiers abstracts et articles par identifiant\n",
        "for identifiant in abstracts:\n",
        "    if identifiant in articles:\n",
        "        # Ajouter l'appariement au dictionnaire\n",
        "        appariements[identifiant] = {\n",
        "            \"abstract\": abstracts[identifiant],\n",
        "            \"article\": articles[identifiant]\n",
        "        }\n",
        "    else:\n",
        "        # Ajouter à la liste des abstracts non appariés\n",
        "        non_apparies_abstracts.append(abstracts[identifiant])\n",
        "\n",
        "# Vérifier les articles non-appariés\n",
        "for identifiant in articles:\n",
        "    if identifiant not in abstracts:\n",
        "        # Ajouter à la liste des articles non appariés\n",
        "        non_apparies_articles.append(articles[identifiant])\n",
        "\n",
        "# Affichage des appariements\n",
        "\"\"\"\n",
        "print(\"Appariements :\")\n",
        "for identifiant, fichiers in appariements.items():\n",
        "    print(f\"Identifiant {identifiant}:\")\n",
        "    print(f\"  Abstract: {fichiers['abstract']}\")\n",
        "    print(f\"  Article: {fichiers['article']}\")\n",
        "\"\"\"\n",
        "# Affichage des abstracts non-appariés\n",
        "print(\"\\nAbstracts non-appariés :\")\n",
        "for abstract in non_apparies_abstracts:\n",
        "    print(abstract)\n",
        "\n",
        "# Affichage des articles non-appariés\n",
        "print(\"\\nArticles non-appariés :\")\n",
        "for article in non_apparies_articles:\n",
        "    print(article)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_5wciJ57jnhk",
        "outputId": "b9096f10-f7a2-4209-fc66-42350f644ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Abstracts non-appariés :\n",
            "abstract-37796016.txt\n",
            "abstract-24279685.txt\n",
            "abstract-36404343.txt\n",
            "abstract-37833688.txt\n",
            "abstract-37102598.txt\n",
            "abstract-27423055.txt\n",
            "abstract-35324507.txt\n",
            "abstract-32946618.txt\n",
            "abstract-32017677.txt\n",
            "abstract-38022520.txt\n",
            "abstract-32091358.txt\n",
            "abstract-36951168.txt\n",
            "abstract-36066965.txt\n",
            "abstract-36891751.txt\n",
            "abstract-37614109.txt\n",
            "abstract-36314570.txt\n",
            "abstract-37026745.txt\n",
            "abstract-36519326.txt\n",
            "abstract-36944050.txt\n",
            "abstract-36944082.txt\n",
            "abstract-37726286.txt\n",
            "abstract-35081022.txt\n",
            "abstract-36068298.txt\n",
            "abstract-36372692.txt\n",
            "abstract-31625835.txt\n",
            "abstract-36525381.txt\n",
            "abstract-36944043.txt\n",
            "\n",
            "Articles non-appariés :\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A noter que quelques résumés n'ont pas d'article qui leur correspond !"
      ],
      "metadata": {
        "id": "ySTyiFhjd0i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On retravaille un peu la forme de ce dictionnaire pour pouvoir le convertir au format \"dataset\" de Hugging face, nécessaire pour pouvoir entraîner le modèle choisi dessus.\n"
      ],
      "metadata": {
        "id": "7axwPxrAd7Di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets #le \"-U\" demande l'installation de la toute dernière version du package\n",
        "from datasets import Dataset #importe la classe Dataset de la bibliothèque datasets de Hugging Face.\n",
        "#Cette classe est utilisée pour manipuler des ensembles de données dans un format qui peut être utilisé pour l'entraînement de modèles issus d'Hugging Face.\n",
        "\n",
        "#Notre dictionnaire appariements n'est pas tout à fait sous la bonne forme pour être transformé en dataset de Hugging Face :\n",
        "#on le remanie donc pour qu'il ait cette bonne forme.\n",
        "\n",
        "# Initialisation des listes vides\n",
        "identifiants = []\n",
        "articles = []\n",
        "abstracts = []\n",
        "\n",
        "# Remplir les listes avec les données extraites du dictionnaire appariements\n",
        "for identifiant, values in appariements.items():\n",
        "    identifiants.append(identifiant)         # Ajout de l'identifiant\n",
        "    articles.append(values[\"article\"])      # Ajout de l'article\n",
        "    abstracts.append(values[\"abstract\"])    # Ajout de l'abstract\n",
        "\n",
        "# Créer un dictionnaire avec les listes\n",
        "data = {\n",
        "    \"identifiant\": identifiants,\n",
        "    \"article\": articles,\n",
        "    \"abstract\": abstracts\n",
        "}\n",
        "\n",
        "# Créer le Dataset en convertissant notre nouveau dictionnaire via l'instruction Dataset.from_dict() de Dataset\n",
        "dataset = Dataset.from_dict(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wJ-fvqMz7ahj",
        "outputId": "5cc78080-3b1a-48ba-9db2-27872f4ec33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On convertit maintenant le texte des articles et des résumés en séquences de tokens."
      ],
      "metadata": {
        "id": "eDcpedpneMHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour préparer les données (tokenisation)\n",
        "def tokenize_function(examples):\n",
        "    # Tokeniser les articles et les résumés\n",
        "    inputs = tokenizer(examples['article'], truncation=True, padding=\"max_length\", max_length=1024) #données en entrée\n",
        "    #examples['article'] : C'est la donnée brute (l'article) fournie à la fonction sous forme de texte.\n",
        "    #Cette donnée vient d'un Dataset ou d'une DataLoader et est passée sous forme de liste de textes (articles).\n",
        "    #tokenizer(examples['article']) : Cette ligne utilise le tokenizer pour convertir le texte brut des articles en une séquence de tokens.\n",
        "    #Le tokenizer transforme le texte en un format compréhensible par le modèle (i.e., des IDs de tokens).\n",
        "\n",
        "    #truncation=True : Si le texte est plus long que la longueur maximale définie, il sera tronqué pour correspondre à cette longueur maximale.\n",
        "\n",
        "    #padding=\"max_length\" : Cela permet de remplir (padd) le texte pour qu'il atteigne la longueur maximale spécifiée.\n",
        "    #Si un article est plus court que la longueur maximale, des tokens de remplissage seront ajoutés.\n",
        "\n",
        "    #max_length=1024 : La longueur maximale des séquences d'entrée est fixée à 1024 tokens. Si un article est plus long que cela, il sera tronqué à 1024 tokens.\n",
        "\n",
        "    targets = tokenizer(examples['abstract'], truncation=True, padding=\"max_length\", max_length=256)#données cible de l'entrainement du modèle\n",
        "\n",
        "    # Retourner les inputs et targets sous la forme de dictionnaires\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    #inputs[\"labels\"] : Dans un modèle de génération de texte comme BigBirdPegasusForConditionalGeneration, les labels sont les séquences que le modèle doit prédire\n",
        "    #(les résumés dans ce cas). En d'autres termes, le modèle apprend à prédire les tokens du résumé à partir des tokens de l'article.\n",
        "    #targets[\"input_ids\"] : input_ids est la représentation des tokens du résumé (les IDs numériques des tokens). Ces tokens sont utilisés comme labels lors de l'entraînement,\n",
        "    #c'est-à-dire que le modèle essaiera de prédire ces input_ids lorsqu'il verra les tokens des articles.\n",
        "    #Cette ligne ajoute donc les input_ids des résumés dans la clé \"labels\" des données d'entrée, qui est utilisée pendant l'entraînement pour calculer la perte\n",
        "    #entre les prédictions du modèle et les résumés réels.\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "41fMtp6-pHgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On paramètre la métrique \"ROUGE\", qui est un score permettant de mesurer la similarité entre les résumés générés et les résumés réels."
      ],
      "metadata": {
        "id": "yuZjyuYRFQnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U evaluate\n",
        "!pip install rouge_score\n",
        "from evaluate import load # Import load_metric from evaluate instead of datasets\n",
        "\n",
        "# Charger la métrique ROUGE\n",
        "rouge_metric = load(\"rouge\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    # Décoder les prédictions et les labels pour obtenir des chaînes de caractères\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Calculer la ROUGE\n",
        "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    # Retourner les résultats sous forme de dictionnaire\n",
        "    return result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DeBAcK5YFML0",
        "outputId": "6b61bed0-801b-45e5-d2cf-03519097d5c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement du modèle bigbird-pegasus-large-arxiv\n",
        "\n"
      ],
      "metadata": {
        "id": "TxbTu-oNDOC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce modèle a pour singularité, par rapport à des modèles de type BERT, d'utiliser une version modifiée de l'architecture Transformer : il a un mécanisme d'attention sparse (attention réduite) par blocs. Concrètement, cela signifie qu'au lieu de calculer l'attention entre toutes les positions du texte, il divise la séquence (le texte) en blocs et applique l'attention uniquement au sein de ces blocs, ainsi qu'entre certains blocs (par exemple les blocs voisins ou un échantillonnage aléatoire de blocs).\n",
        "\n",
        "Cela permet d'être économe en temps de calcul : avec un modèle de type BERT, ce dernier croissait en $O(N^2)$, où $N$ représente la taille du texte (ou document). Avec le mécanisme d'attention sparse par blocs, on tombe à des temps de calcul en $O(N.log(N))$ ou en $O(N)$.\n",
        "\n",
        "Pour nos articles scientifiques, qui sont des documents longs, ce type de modèles, avec attention sparse par blocs, peut donc être particulièrement pertinent."
      ],
      "metadata": {
        "id": "fUiZk0D9q-c1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1+cu117 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip uninstall -y tensorflow\n",
        "from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer # Import TrainingArguments and Trainer\n",
        "\n",
        "#Instancier le tokenizer pour le modèle pegasus\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
        "\n",
        "#chargement du modèle :\n",
        "# by default encoder-attention is `block_sparse` with num_random_blocks=3, block_size=64/par défaut l'encoder-attention est \"block sparse\"\n",
        "#avec num_random_blocks=3 et block_size=64\n",
        "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\",\n",
        "                                                               return_dict=True,\n",
        "                                                               torch_dtype=torch.float16,\n",
        "                                                               device_map=\"auto\")\n",
        "\n",
        "######################Options pour modifier la taille et le nombre des blocs d'attention :\n",
        "# decoder attention type can't be changed & will be \"original_full\"\n",
        "# you can change `attention_type` (encoder only) to full attention like this:\n",
        "##model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", attention_type=\"original_full\")\n",
        "\n",
        "# you can change `block_size` & `num_random_blocks` like this:\n",
        "##model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", block_size=16, num_random_blocks=2)\n",
        "######################\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1WNnycYXvQM",
        "outputId": "89d58390-0900-4b3b-f348-a00bb02a5e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.13.1+cu117 in /usr/local/lib/python3.10/dist-packages (1.13.1+cu117)\n",
            "Requirement already satisfied: torchvision==0.14.1+cu117 in /usr/local/lib/python3.10/dist-packages (0.14.1+cu117)\n",
            "Requirement already satisfied: torchaudio==0.13.1+cu117 in /usr/local/lib/python3.10/dist-packages (0.13.1+cu117)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1+cu117) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (11.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2024.12.14)\n",
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On paramètre l'entraînement du modèle."
      ],
      "metadata": {
        "id": "s9i3nuomeifY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Appliquer la tokenisation\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Mélanger et diviser le dataset tokenisé, en 90% pour l'entraînement et 10% pour la validation\n",
        "train_dataset, val_dataset = tokenized_datasets.train_test_split(test_size=0.1).values()\n",
        "\n",
        "# Définir les arguments d'entraînement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bigbird_pegasus_finetuned\",  # Répertoire de sortie pour enregistrer les résultats\n",
        "    evaluation_strategy=\"epoch\",  # Évaluer après chaque époque\n",
        "    learning_rate=2e-5,  # Taux d'apprentissage\n",
        "    per_device_train_batch_size=2,  # Taille du batch pour l'entraînement\n",
        "    per_device_eval_batch_size=2,   # Taille du batch pour l'évaluation\n",
        "    num_train_epochs=3,  # Nombre d'époques d'entraînement\n",
        "    weight_decay=0.01,  # L2 regularization\n",
        "    save_total_limit=2,  # Limite du nombre de sauvegardes du modèle\n",
        "    logging_dir=\"./logs\",  # Répertoire pour les logs\n",
        "    logging_steps=10,  # Fréquence des logs\n",
        "    report_to=\"tensorboard\",  # Optionnel : Utiliser TensorBoard pour la visualisation des métriques\n",
        ")\n",
        "\n",
        "# Créer un objet Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,  # Le modèle à fine-tuner\n",
        "    args=training_args,  # Les arguments d'entraînement\n",
        "    train_dataset=train_dataset,  # Jeu d'entraînement\n",
        "    eval_dataset=val_dataset,  # Jeu de validation\n",
        "   # compute_metrics=compute_metrics,  # Fonction pour calculer les métriques - elle nous permettra d'utiliser le score \"ROUGE\"\n",
        ")\n",
        "\n",
        "# Lancer l'entraînement\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "# Sauvegarder le modèle fine-tuné\n",
        "trainer.save_model(\"./bigbird_pegasus_finetuned\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452,
          "referenced_widgets": [
            "590d7930119a40ec80852b61f1d745f1",
            "c7494184df6a4d76be440b3aa8e450ed",
            "20f371d49459494dab86ffabc022bd8b",
            "6f42d7009347405b9f171086ff5d006f",
            "e15000d777124cf1a0d716f9a5dfaf94",
            "087cef66d27d488aaaaed55dd832b0e8",
            "8c661e2b93de4efeaef3280a2fec9e24",
            "1c18877d0dc04869878441ec29cebda9",
            "1895bff15c9a4779a1e15f01e88dd63d",
            "f47d699049e94e018ef36107abf72b43",
            "4eb60c52894a421ca2644f12b86a1a62"
          ]
        },
        "id": "9_UU-nmJqoiQ",
        "outputId": "08af7291-0207-4cbf-b546-3431e5c94376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/402 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "590d7930119a40ec80852b61f1d745f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "\"bernoulli_scalar_cpu_\" not implemented for 'Half'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-40a83a10c506>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Lancer l'entraînement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2522\u001b[0m                     )\n\u001b[1;32m   2523\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2524\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3653\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3654\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3656\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3706\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3707\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3708\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3709\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3710\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2519\u001b[0m                 )\n\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2521\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   2522\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2387\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   2388\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2389\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membed_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \"bernoulli_scalar_cpu_\" not implemented for 'Half'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluer la performance du modèle sur l'ensemble de validation\n",
        "eval_results = trainer.evaluate(eval_dataset=val_dataset)\n",
        "\n",
        "# Afficher les résultats d'évaluation\n",
        "print(eval_results)\n"
      ],
      "metadata": {
        "id": "12ta6aFOENdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chargement du modèle Bart-large-CNN"
      ],
      "metadata": {
        "id": "NzTIaaq8NR_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BART est un modèle de transformer encodeur-décodeur (seq2seq : \"sequence to séquence\") avec un encodeur bidirectionnel (similaire à BERT) et un décodeur autoregressif (similaire à GPT).\n",
        "Il est pré-entraîné en masquant des parties du texte et en apprenant à prédire ces parties - comme si on lui demandait de débruiter un texte bruité.\n",
        "Il utilise une attention dense classique pour un transformer, où chaque token de l'entrée prend en compte tous les autres tokens de la séquence."
      ],
      "metadata": {
        "id": "_ogvtW3usO7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#A faire"
      ],
      "metadata": {
        "id": "71BHna2eNf7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi #afficher des informations sur les GPU NVIDIA disponibles sur la machine"
      ],
      "metadata": {
        "id": "aVhffGQjyN4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d50d64c-78ec-432b-c191-ef56c35619f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    }
  ]
}