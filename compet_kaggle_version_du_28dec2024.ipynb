{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 90248,
          "databundleVersionId": 10480827,
          "sourceType": "competition"
        },
        {
          "sourceId": 212098,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 180807,
          "modelId": 203049
        }
      ],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Confection de résumés automatiques d'articles scientifiques par un modèle de langue pré-entraîné : étude et comparaison des performances de deux modèles.\n",
        "\n",
        "**Auteur.e.s :** Sophie perrin, Emma, El Zube, Marius, Sylvano, pour le cours *Representation learning for NLP* @ Master 2 MALIA et MIASHS.\n",
        "\n",
        "**Equipe :** les léopards"
      ],
      "metadata": {
        "id": "Jm3aCEp_w2w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Préparation de l'environnement\n",
        "\n",
        "1. Se connecter ou se créer un compte sur https://huggingface.co/\n",
        "2. Se créer un nouveau token d'accès : https://huggingface.co/settings/tokens\n",
        "3. Enregistrer ce token comme un secret nommé `HF_TOKEN` dans Google Colab (icone \"clef\" dans le bandeau vertical)\n",
        "4. Exécuter le code ci-dessous\n",
        "\n",
        "Note importante pour la vitesse de calcul : pour activer l'accélération GPU dans votre notebook Kaggle :\n",
        "\n",
        "    Ouvrez le menu « Settings » : Dans votre notebook Kaggle, cliquez sur le menu « Accelerator » en haut de l'écran, et choisissez un GPU.\n",
        "\n",
        "    Dans ce même menu, vérifiez qu'il n'est pas écrit \"turn off internet\" - sinon cliquer dessus pour rétablir l'accès au reste d'internet depuis kaggle. Si vous n'avez ni \"turn off internet\" ni \"turn on internet\" d'écrit, alors il faut ajouter votre téléphone et votre photo pour certifier votre identité pour le compte, avant de pouvoir accéder au reste d'internet depuis kaggle (nécessaire pour charger les modèles de huggingface et même les packages de python...!).\n",
        "\n"
      ],
      "metadata": {
        "id": "oiCaaZ8YC8hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()"
      ],
      "metadata": {
        "id": "3pp0itMoj78l",
        "outputId": "6fc6780e-9cd4-447f-c06d-f7f325e10549",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T16:11:16.635146Z",
          "iopub.execute_input": "2024-12-28T16:11:16.635399Z",
          "iopub.status.idle": "2024-12-28T16:11:16.921704Z",
          "shell.execute_reply.started": "2024-12-28T16:11:16.635378Z",
          "shell.execute_reply": "2024-12-28T16:11:16.920576Z"
        },
        "colab": {
          "referenced_widgets": [
            "95322fb8fab547d4bd4a03939cf185c2"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95322fb8fab547d4bd4a03939cf185c2"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "m_2_maliash_resume_darticles_scientifiques_path = kagglehub.competition_download('m-2-maliash-resume-darticles-scientifiques')\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "id": "z-S3zGw4BfUX",
        "outputId": "b91c7e68-5d81-41a5-e693-63e126680a97",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T16:13:47.922763Z",
          "iopub.execute_input": "2024-12-28T16:13:47.923047Z",
          "iopub.status.idle": "2024-12-28T16:13:48.833929Z",
          "shell.execute_reply.started": "2024-12-28T16:13:47.923024Z",
          "shell.execute_reply": "2024-12-28T16:13:48.833191Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Data source import complete.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Pour créer le \"HF_TOKEN\" dans Kaggle, aller dans \"Add-ons\" (dans les menus de ce notebook, juste à gauche de \"Help\").\n",
        "#Une fois là, aller dans \"secrets\", puis ça marche à peu près comme dans colab : il faut nommer la variable (HF_TOKEN) et\n",
        "#y copier sa valeur.\n",
        "\n",
        "\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T16:13:50.181926Z",
          "iopub.execute_input": "2024-12-28T16:13:50.182216Z",
          "iopub.status.idle": "2024-12-28T16:13:50.358950Z",
          "shell.execute_reply.started": "2024-12-28T16:13:50.182194Z",
          "shell.execute_reply": "2024-12-28T16:13:50.358317Z"
        },
        "id": "nOrgjrKlla0i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Préparation des données d'entraînement pour leur utilisation par le modèle"
      ],
      "metadata": {
        "id": "hoSW39T9djGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Données du fichier OBS"
      ],
      "metadata": {
        "id": "rL3prK3hla0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée un dictionnaire qui apparie les articles de l'ensemble d'entraînement (\"fine tuning\" puisque le modèle est déjà pré-entraîné) et leurs résumés par leur identifiant commun."
      ],
      "metadata": {
        "id": "iSOUl2N1dtAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Chemins dans Kaggle des dossiers contenant les fichiers\n",
        "dossier_abstracts = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/abstracts_OBS/\"\n",
        "dossier_articles = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/articles_OBS/\"\n",
        "\n",
        "\n",
        "# Liste des fichiers dans chaque dossier\n",
        "fichiers_abstracts = [f for f in os.listdir(dossier_abstracts) if f.startswith(\"abstract-\")]\n",
        "fichiers_articles = [f for f in os.listdir(dossier_articles) if f.startswith(\"article-\")]\n",
        "\n",
        "# Dictionnaires pour stocker les fichiers par identifiant\n",
        "abstracts = {}\n",
        "articles = {}\n",
        "\n",
        "# Remplir les dictionnaires avec les fichiers en fonction des identifiants\n",
        "for fichier in fichiers_abstracts:\n",
        "    # Extraire l'identifiant du fichier abstract\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "     #fichier.split(\"-\") : La méthode split(\"-\") divise le nom du fichier\n",
        "    #en une liste de sous-chaînes en utilisant le caractère \"-\" comme séparateur.\n",
        "    #Par exemple, si le fichier est \"abstract-123.txt\", fichier.split(\"-\") renverra la liste [\"abstract\", \"123.txt\"].\n",
        "    #fichier.split(\"-\")[1] : En prenant l'élément d'indice 1 de cette liste (c'est-à-dire \"123.txt\"), on obtient la partie du nom du fichier après le préfixe \"abstract-\".\n",
        "    #split(\".\")[0] : Ensuite, on divise cette chaîne \"123.txt\" avec split(\".\"), ce qui donne [\"123\", \"txt\"].\n",
        "    #On prend le premier élément de la liste (c'est-à-dire \"123\") qui est l'identifiant unique de l'abstract.\n",
        "    abstracts[identifiant] = fichier\n",
        "    #Cette ligne ajoute une entrée dans le dictionnaire abstracts, où la clé est : identifiant (par exemple \"123\")\n",
        "    #et la valeur est le nom du fichier : fichier (par exemple \"abstract-123.txt\").\n",
        "\n",
        "for fichier in fichiers_articles:\n",
        "    # Extraire l'identifiant du fichier article\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "    articles[identifiant] = fichier\n",
        "\n",
        "# Dictionnaire pour stocker les appariements\n",
        "appariements = {}\n",
        "\n",
        "# Liste des abstracts et articles non-appariés\n",
        "non_apparies_abstracts = []\n",
        "non_apparies_articles = []\n",
        "\n",
        "# Appariement des fichiers abstracts et articles par identifiant\n",
        "for identifiant in abstracts:\n",
        "    if identifiant in articles:\n",
        "        # Ajouter l'appariement au dictionnaire\n",
        "        appariements[identifiant] = {\n",
        "            \"abstract\": abstracts[identifiant],\n",
        "            \"article\": articles[identifiant]\n",
        "        }\n",
        "    else:\n",
        "        # Ajouter à la liste des abstracts non appariés\n",
        "        non_apparies_abstracts.append(abstracts[identifiant])\n",
        "\n",
        "# Vérifier les articles non-appariés\n",
        "for identifiant in articles:\n",
        "    if identifiant not in abstracts:\n",
        "        # Ajouter à la liste des articles non appariés\n",
        "        non_apparies_articles.append(articles[identifiant])\n",
        "\n",
        "# Affichage des appariements\n",
        "\"\"\"\n",
        "print(\"Appariements :\")\n",
        "for identifiant, fichiers in appariements.items():\n",
        "    print(f\"Identifiant {identifiant}:\")\n",
        "    print(f\"  Abstract: {fichiers['abstract']}\")\n",
        "    print(f\"  Article: {fichiers['article']}\")\n",
        "\"\"\"\n",
        "# Affichage des abstracts non-appariés\n",
        "print(\"\\nAbstracts non-appariés :\")\n",
        "for abstract in non_apparies_abstracts:\n",
        "    print(abstract)\n",
        "\n",
        "# Affichage des articles non-appariés\n",
        "print(\"\\nArticles non-appariés :\")\n",
        "for article in non_apparies_articles:\n",
        "    print(article)"
      ],
      "metadata": {
        "id": "_5wciJ57jnhk",
        "outputId": "b9096f10-f7a2-4209-fc66-42350f644ec5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T00:27:00.178159Z",
          "iopub.execute_input": "2024-12-28T00:27:00.178442Z",
          "iopub.status.idle": "2024-12-28T00:27:00.213808Z",
          "shell.execute_reply.started": "2024-12-28T00:27:00.178423Z",
          "shell.execute_reply": "2024-12-28T00:27:00.213155Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nAbstracts non-appariés :\nabstract-36404343.txt\nabstract-38022520.txt\nabstract-37614109.txt\nabstract-36944082.txt\nabstract-32017677.txt\nabstract-36944050.txt\nabstract-36891751.txt\nabstract-35081022.txt\nabstract-32091358.txt\nabstract-31625835.txt\nabstract-36066965.txt\nabstract-36519326.txt\nabstract-36525381.txt\nabstract-36314570.txt\nabstract-37833688.txt\nabstract-27423055.txt\nabstract-37796016.txt\nabstract-37026745.txt\nabstract-36068298.txt\nabstract-35324507.txt\nabstract-37726286.txt\nabstract-37102598.txt\nabstract-36951168.txt\nabstract-24279685.txt\nabstract-36944043.txt\nabstract-32946618.txt\nabstract-36372692.txt\n\nArticles non-appariés :\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "A noter que quelques résumés n'ont pas d'article qui leur correspond !"
      ],
      "metadata": {
        "id": "ySTyiFhjd0i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On retravaille un peu la forme de ce dictionnaire pour pouvoir le convertir au format \"dataset\" de Hugging face, nécessaire pour pouvoir entraîner le modèle choisi dessus.\n"
      ],
      "metadata": {
        "id": "7axwPxrAd7Di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets #le \"-U\" demande l'installation de la toute dernière version du package\n",
        "from datasets import Dataset #importe la classe Dataset de la bibliothèque datasets de Hugging Face.\n",
        "#Cette classe est utilisée pour manipuler des ensembles de données dans un format qui peut être utilisé pour l'entraînement de modèles issus d'Hugging Face.\n",
        "\n",
        "# Limiter le nombre de threads de JAX à 1 (pour éviter les problèmes dans kaggle)\n",
        "os.environ[\"JAX_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "#Notre dictionnaire appariements n'est pas tout à fait sous la bonne forme pour être transformé en dataset de Hugging Face :\n",
        "#on le remanie donc pour qu'il ait cette bonne forme.\n",
        "\n",
        "# Initialisation des listes vides\n",
        "identifiants = []\n",
        "articles = []\n",
        "abstracts = []\n",
        "\n",
        "# Remplir les listes avec les données extraites du dictionnaire appariements\n",
        "for identifiant, values in appariements.items():\n",
        "    identifiants.append(identifiant)         # Ajout de l'identifiant\n",
        "    articles.append(values[\"article\"])      # Ajout de l'article\n",
        "    abstracts.append(values[\"abstract\"])    # Ajout de l'abstract\n",
        "\n",
        "# Créer un dictionnaire avec les listes\n",
        "data = {\n",
        "    \"identifiant\": identifiants,\n",
        "    \"article\": articles,\n",
        "    \"abstract\": abstracts\n",
        "}\n",
        "\n",
        "# Créer le Dataset en convertissant notre nouveau dictionnaire via l'instruction Dataset.from_dict() de Dataset\n",
        "dataset = Dataset.from_dict(data)\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "wJ-fvqMz7ahj",
        "outputId": "5cc78080-3b1a-48ba-9db2-27872f4ec33c",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T00:27:05.066278Z",
          "iopub.execute_input": "2024-12-28T00:27:05.066565Z",
          "iopub.status.idle": "2024-12-28T00:27:10.908628Z",
          "shell.execute_reply.started": "2024-12-28T00:27:05.066544Z",
          "shell.execute_reply": "2024-12-28T00:27:10.907796Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDataset({\n    features: ['identifiant', 'article', 'abstract'],\n    num_rows: 402\n})\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## le fichier à utiliser pour l'entraînement du modèle sur les articles de type OBS sera donc \"dataset\""
      ],
      "metadata": {
        "id": "0oFbiBnWla0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Données RCT"
      ],
      "metadata": {
        "id": "O1JUk4V6la0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On procède de même pour les données des articles de type \"RCT\""
      ],
      "metadata": {
        "id": "pfykvJwpla0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Chemins dans Kaggle des dossiers contenant les fichiers\n",
        "dossier_abstracts = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/RCT/abstracts_RCT/\"\n",
        "dossier_articles = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/RCT/articles_RCT/\"\n",
        "\n",
        "\n",
        "# Liste des fichiers dans chaque dossier\n",
        "fichiers_abstracts = [f for f in os.listdir(dossier_abstracts) if f.startswith(\"abstract-\")]\n",
        "fichiers_articles = [f for f in os.listdir(dossier_articles) if f.startswith(\"article-\")]\n",
        "\n",
        "# Dictionnaires pour stocker les fichiers par identifiant\n",
        "abstracts = {}\n",
        "articles = {}\n",
        "\n",
        "# Remplir les dictionnaires avec les fichiers en fonction des identifiants\n",
        "for fichier in fichiers_abstracts:\n",
        "    # Extraire l'identifiant du fichier abstract\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "     #fichier.split(\"-\") : La méthode split(\"-\") divise le nom du fichier\n",
        "    #en une liste de sous-chaînes en utilisant le caractère \"-\" comme séparateur.\n",
        "    #Par exemple, si le fichier est \"abstract-123.txt\", fichier.split(\"-\") renverra la liste [\"abstract\", \"123.txt\"].\n",
        "    #fichier.split(\"-\")[1] : En prenant l'élément d'indice 1 de cette liste (c'est-à-dire \"123.txt\"), on obtient la partie du nom du fichier après le préfixe \"abstract-\".\n",
        "    #split(\".\")[0] : Ensuite, on divise cette chaîne \"123.txt\" avec split(\".\"), ce qui donne [\"123\", \"txt\"].\n",
        "    #On prend le premier élément de la liste (c'est-à-dire \"123\") qui est l'identifiant unique de l'abstract.\n",
        "    abstracts[identifiant] = fichier\n",
        "    #Cette ligne ajoute une entrée dans le dictionnaire abstracts, où la clé est : identifiant (par exemple \"123\")\n",
        "    #et la valeur est le nom du fichier : fichier (par exemple \"abstract-123.txt\").\n",
        "\n",
        "for fichier in fichiers_articles:\n",
        "    # Extraire l'identifiant du fichier article\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "    articles[identifiant] = fichier\n",
        "\n",
        "# Dictionnaire pour stocker les appariements\n",
        "appariements_RCT = {}\n",
        "\n",
        "# Liste des abstracts et articles non-appariés\n",
        "non_apparies_abstracts_RCT = []\n",
        "non_apparies_articles_RCT = []\n",
        "\n",
        "# Appariement des fichiers abstracts et articles par identifiant\n",
        "for identifiant in abstracts:\n",
        "    if identifiant in articles:\n",
        "        # Ajouter l'appariement au dictionnaire\n",
        "        appariements_RCT[identifiant] = {\n",
        "            \"abstract\": abstracts[identifiant],\n",
        "            \"article\": articles[identifiant]\n",
        "        }\n",
        "    else:\n",
        "        # Ajouter à la liste des abstracts non appariés\n",
        "        non_apparies_abstracts_RCT.append(abstracts[identifiant])\n",
        "\n",
        "# Vérifier les articles non-appariés\n",
        "for identifiant in articles:\n",
        "    if identifiant not in abstracts:\n",
        "        # Ajouter à la liste des articles non appariés\n",
        "        non_apparies_articles_RCT.append(articles[identifiant])\n",
        "\n",
        "# Affichage des appariements\n",
        "\"\"\"\n",
        "print(\"Appariements_RCT :\")\n",
        "for identifiant, fichiers in appariements_RCT.items():\n",
        "    print(f\"Identifiant {identifiant}:\")\n",
        "    print(f\"  Abstract: {fichiers['abstract']}\")\n",
        "    print(f\"  Article: {fichiers['article']}\")\n",
        "\"\"\"\n",
        "# Affichage des abstracts non-appariés\n",
        "print(\"\\nAbstracts non-appariés :\")\n",
        "for abstract in non_apparies_abstracts_RCT:\n",
        "    print(abstract)\n",
        "\n",
        "# Affichage des articles non-appariés\n",
        "print(\"\\nArticles non-appariés :\")\n",
        "for article in non_apparies_articles_RCT:\n",
        "    print(article)"
      ],
      "metadata": {
        "trusted": true,
        "id": "i9zDXRwyla0o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets #le \"-U\" demande l'installation de la toute dernière version du package\n",
        "from datasets import Dataset #importe la classe Dataset de la bibliothèque datasets de Hugging Face.\n",
        "#Cette classe est utilisée pour manipuler des ensembles de données dans un format qui peut être utilisé pour l'entraînement de modèles issus d'Hugging Face.\n",
        "\n",
        "# Limiter le nombre de threads de JAX à 1 (pour éviter les problèmes dans kaggle)\n",
        "os.environ[\"JAX_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "#Notre dictionnaire appariements_RCT n'est pas tout à fait sous la bonne forme pour être transformé en dataset de Hugging Face :\n",
        "#on le remanie donc pour qu'il ait cette bonne forme.\n",
        "\n",
        "# Initialisation des listes vides\n",
        "identifiants = []\n",
        "articles = []\n",
        "abstracts = []\n",
        "\n",
        "# Remplir les listes avec les données extraites du dictionnaire appariements\n",
        "for identifiant, values in appariements_RCT.items():\n",
        "    identifiants.append(identifiant)         # Ajout de l'identifiant\n",
        "    articles.append(values[\"article\"])      # Ajout de l'article\n",
        "    abstracts.append(values[\"abstract\"])    # Ajout de l'abstract\n",
        "\n",
        "# Créer un dictionnaire avec les listes\n",
        "data = {\n",
        "    \"identifiant\": identifiants,\n",
        "    \"article\": articles,\n",
        "    \"abstract\": abstracts\n",
        "}\n",
        "\n",
        "# Créer le Dataset en convertissant notre nouveau dictionnaire via l'instruction Dataset.from_dict() de Dataset\n",
        "dataset_RCT = Dataset.from_dict(data)\n",
        "\n",
        "#print(dataset_RCT)"
      ],
      "metadata": {
        "trusted": true,
        "id": "9foOVsZwla0o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Le fichier à utiliser pour l'entrainement sur les données RCT sera donc \"dataset_RCT\""
      ],
      "metadata": {
        "id": "2qTVuUuKla0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Essai du modèle bigbird-pegasus-large-arxiv"
      ],
      "metadata": {
        "id": "Z_GMNlNula0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce modèle a pour singularité, par rapport à des modèles de type BERT, d'utiliser une version modifiée de l'architecture Transformer : il a un mécanisme d'attention sparse (attention réduite) par blocs. Concrètement, cela signifie qu'au lieu de calculer l'attention entre toutes les positions du texte, il divise la séquence (le texte) en blocs et applique l'attention uniquement au sein de ces blocs, ainsi qu'entre certains blocs (par exemple les blocs voisins ou un échantillonnage aléatoire de blocs).\n",
        "\n",
        "Cela permet d'être économe en temps de calcul : avec un modèle de type BERT, ce dernier croissait en $O(N^2)$, où $N$ représente la taille du texte (ou document). Avec le mécanisme d'attention sparse par blocs, on tombe à des temps de calcul en $O(N.log(N))$ ou en $O(N)$.\n",
        "\n",
        "Pour nos articles scientifiques, qui sont des documents longs, ce type de modèles, avec attention sparse par blocs, peut donc être particulièrement pertinent."
      ],
      "metadata": {
        "id": "fUiZk0D9q-c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On convertit maintenant le texte des articles et des résumés (ici ceux de type \"OBS\") en séquences de tokens pour le modèle bigbird-pegasus-large-arxiv."
      ],
      "metadata": {
        "id": "eDcpedpneMHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour préparer les données (tokenisation) pour un modèle chargé depuis huggingface\n",
        "def tokenize_function(examples):\n",
        "    # Tokeniser les articles et les résumés\n",
        "    inputs = tokenizer(examples['article'], truncation=True, padding=\"max_length\", max_length=1024) #données en entrée\n",
        "    #examples['article'] : C'est la donnée brute (l'article) fournie à la fonction sous forme de texte.\n",
        "    #Cette donnée vient d'un Dataset ou d'une DataLoader et est passée sous forme de liste de textes (articles).\n",
        "    #tokenizer(examples['article']) : Cette ligne utilise le tokenizer pour convertir le texte brut des articles en une séquence de tokens.\n",
        "    #Le tokenizer transforme le texte en un format compréhensible par le modèle (i.e., des IDs de tokens).\n",
        "\n",
        "    #truncation=True : Si le texte est plus long que la longueur maximale définie, il sera tronqué pour correspondre à cette longueur maximale.\n",
        "\n",
        "    #padding=\"max_length\" : Cela permet de remplir (padd) le texte pour qu'il atteigne la longueur maximale spécifiée.\n",
        "    #Si un article est plus court que la longueur maximale, des tokens de remplissage seront ajoutés.\n",
        "\n",
        "    #max_length=1024 : La longueur maximale des séquences d'entrée est fixée à 1024 tokens. Si un article est plus long que cela, il sera tronqué à 1024 tokens.\n",
        "\n",
        "    targets = tokenizer(examples['abstract'], truncation=True, padding=\"max_length\", max_length=256)#données cible de l'entrainement du modèle\n",
        "\n",
        "    # Retourner les inputs et targets sous la forme de dictionnaires\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    #inputs[\"labels\"] : Dans un modèle de génération de texte comme BigBirdPegasus, les labels sont les séquences que le modèle doit prédire\n",
        "    #(les résumés dans ce cas). En d'autres termes, le modèle apprend à prédire les tokens du résumé à partir des tokens de l'article.\n",
        "    #targets[\"input_ids\"] : input_ids est la représentation des tokens du résumé (les IDs numériques des tokens). Ces tokens sont utilisés comme labels lors de l'entraînement,\n",
        "    #c'est-à-dire que le modèle essaiera de prédire ces input_ids lorsqu'il verra les tokens des articles.\n",
        "    #Cette ligne ajoute donc les input_ids des résumés dans la clé \"labels\" des données d'entrée, qui est utilisée pendant l'entraînement pour calculer la perte\n",
        "    #entre les prédictions du modèle et les résumés réels.\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "41fMtp6-pHgH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-27T20:31:18.348955Z",
          "iopub.execute_input": "2024-12-27T20:31:18.349239Z",
          "iopub.status.idle": "2024-12-27T20:31:18.353920Z",
          "shell.execute_reply.started": "2024-12-27T20:31:18.349219Z",
          "shell.execute_reply": "2024-12-27T20:31:18.353031Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement du modèle bigbird-pegasus-large-arxiv\n",
        "\n"
      ],
      "metadata": {
        "id": "TxbTu-oNDOC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer # Import TrainingArguments and Trainer\n",
        "\n",
        "#Instancier le tokenizer pour le modèle pegasus\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/bigbird-pegasus-large-arxiv/\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
        "#chargement du modèle :\n",
        "# by default encoder-attention is `block_sparse` with num_random_blocks=3, block_size=64/par défaut l'encoder-attention est \"block sparse\"\n",
        "#avec num_random_blocks=3 et block_size=64\n",
        "#model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"/kaggle/input/bigbird-pegasus-large-arxiv/\",\n",
        "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\",\n",
        "                                                               return_dict=True,\n",
        "                                                               torch_dtype=torch.float16,\n",
        "                                                               device_map=\"auto\")\n",
        "\n",
        "######################Options pour modifier la taille et le nombre des blocs d'attention :\n",
        "# decoder attention type can't be changed & will be \"original_full\"\n",
        "# you can change `attention_type` (encoder only) to full attention like this:\n",
        "##model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", attention_type=\"original_full\")\n",
        "\n",
        "# you can change `block_size` & `num_random_blocks` like this:\n",
        "##model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", block_size=16, num_random_blocks=2)\n",
        "######################\n",
        "\n"
      ],
      "metadata": {
        "id": "j1WNnycYXvQM",
        "outputId": "89d58390-0900-4b3b-f348-a00bb02a5e7c",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-27T00:28:45.486060Z",
          "iopub.execute_input": "2024-12-27T00:28:45.486576Z",
          "iopub.status.idle": "2024-12-27T00:29:10.018487Z",
          "shell.execute_reply.started": "2024-12-27T00:28:45.486546Z",
          "shell.execute_reply": "2024-12-27T00:29:10.017849Z"
        },
        "colab": {
          "referenced_widgets": [
            "3139ebd1e216433596ad25ee89cf008f",
            "37c2391201ad4c07bb91b64fafe52e89"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/2.31G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3139ebd1e216433596ad25ee89cf008f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/232 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37c2391201ad4c07bb91b64fafe52e89"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrainement du modèle bigbird-pegasus-large-arxiv et conclusions sur ce modèle pour notre compétition kaggle"
      ],
      "metadata": {
        "id": "b5aidQKJla0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On paramètre l'entraînement du modèle.\n",
        "\n",
        "Problème : même en modifiant les paramètres de training_args comme ci-dessous, pour tenter de consommer moins de mémoire, l'entraînement de ce modèle fait planter la session kaggle car il consomme trop de mémoire - y compris en utilisant les GPU 100, ceux qui comportent le plus de mémoire sur kaggle.\n",
        "\n",
        "Nous allons donc tester un autre modèle, composé par l'assemblage de bart-large-cnn et longformer-base-4096, et bart-large-cnn seul (simplement fine-tuné sur nos données d'entraînement)."
      ],
      "metadata": {
        "id": "s9i3nuomeifY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Appliquer la tokenisation\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Définir les arguments d'entraînement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bigbird_pegasus_finetuned\",  # Répertoire de sortie pour enregistrer les résultats\n",
        "    evaluation_strategy=\"epoch\",  # Évaluer après chaque époque\n",
        "    learning_rate=2e-5,  # Taux d'apprentissage\n",
        "    per_device_train_batch_size=1, #2 je réduis pour éviter de planter kaggle,  # Taille du batch pour l'entraînement\n",
        "    per_device_eval_batch_size=1, #2 idem,   # Taille du batch pour l'évaluation\n",
        "    gradient_accumulation_steps=8,  # Accumuler les gradients sur 8 batches avant la mise à jour (réduit l'utilisation de la mémoire, toujours pour éviter de planter kaggle)\n",
        "                                    #cela \"simule\" une taille de batch plus grande que ce qu'elle n'est en réalité ici\n",
        "    fp16=False,  # Activer l'entraînement en 16-bit - toujours pour éviter de planter kaggle\n",
        "    num_train_epochs=3,  # Nombre d'époques d'entraînement\n",
        "    dataloader_num_workers=4,  # Utiliser 4 processus pour charger les données en parallèle\n",
        "    gradient_checkpointing=True,  # Activer gradient checkpointing\n",
        "    save_steps=500,  # Sauvegarder moins souvent\n",
        "    weight_decay=0.01,  # L2 regularization\n",
        "    save_total_limit=2,  # Limite du nombre de sauvegardes du modèle\n",
        "    logging_dir=\"./logs\",  # Répertoire pour les logs\n",
        "    logging_steps=100,  # Fréquence des logs\n",
        "    report_to=\"tensorboard\",  # Optionnel : Utiliser TensorBoard pour la visualisation des métriques\n",
        ")\n",
        "\n",
        "# Créer un objet Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,  # Le modèle à fine-tuner\n",
        "    args=training_args,  # Les arguments d'entraînement\n",
        "    train_dataset=train_dataset,  # Jeu d'entraînement\n",
        "    eval_dataset=val_dataset,  # Jeu de validation\n",
        "    compute_metrics=compute_metrics,  # Fonction pour calculer les métriques - elle nous permettra d'utiliser le score \"ROUGE\"\n",
        ")\n",
        "\n",
        "# Lancer l'entraînement\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "# Sauvegarder le modèle fine-tuné\n",
        "trainer.save_model(\"./bigbird_pegasus_finetuned\")"
      ],
      "metadata": {
        "id": "9_UU-nmJqoiQ",
        "outputId": "08af7291-0207-4cbf-b546-3431e5c94376",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-27T00:29:10.020485Z",
          "iopub.execute_input": "2024-12-27T00:29:10.020922Z",
          "execution_failed": "2024-12-27T00:34:33.966Z"
        },
        "colab": {
          "referenced_widgets": [
            "4f7cb4140108423fa54365a1dcdd7810"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/402 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f7cb4140108423fa54365a1dcdd7810"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='46' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 46/135 04:23 < 08:53, 0.17 it/s, Epoch 1.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='41' max='41' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [41/41 00:08]\n    </div>\n    "
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confection d'un modèle composé à partir de longformer et bart"
      ],
      "metadata": {
        "id": "K67A1zjGla0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement du modèle  allenai/longformer-base-4096\n",
        "\n"
      ],
      "metadata": {
        "id": "EcBwQBTGla0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le modèle bigbird-pegasus-large-arivx demande trop de mémoire pour notre plateforme de compétition kaggle.\n",
        "\n",
        "Nous devons donc nous orienter vers un modèle de compromis, néanmoins adapté au traitement d'articles scientifiques longs.\n",
        "Il nous faut donc chercher dans la famille des modèles à attention \"sparse\", les seuls vraiment adaptés pour ces textes longs.\n",
        "\n",
        "allenai/longformer-large-4096 partage avec bigbird ce mécanisme d'attention sparse : tout comme lui, il utilise une fenêtre d'attention \"glissante\". Tout comme lui, il y ajoute une attention à plus longue portée sur les tokens importants, ce qui lui permet d'interagir avec tous les autres tokens de la séquence pour capturer des relations à long terme. Mais en revanche, contrairement à lui, il n'a pas de mécanisme d'attention aléatoire qui se surajoute à ces deux couches d'attention, et qui lui permet de mieux capter des relations globales...mais consomme de la mémoire vive, lors de son entraînement (fine tuning) notamment.\n",
        "\n",
        "En revanche, il n'est pas un générateur de texte : il nous faut donc l'associer à un modèle de type GPT ou autre qui permet quant à lui la génération. Ce, dans le but de bonifier les performances du modèle générateur par les bonnes performances de longformer sur les textes longs.\n",
        "On l'associe avec le modèle générateur bart-large-cnn, association qu'on crée \"à la main\" via la confection d'une classe spécifique.\n",
        "\n",
        "Comme le modèle ainsi créé reste trop volumineux, on y remplace finalement allenai/longformer-large-4096 par une version plus légère : allenai/longformer-base-4096.\n",
        "\n"
      ],
      "metadata": {
        "id": "6JR1M1Cela0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer # Import TrainingArguments and Trainer\n",
        "\n",
        "# Instancier le tokenizer pour le modèle Longformer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "\n",
        "# Chargement du modèle Longformer\n",
        "# Longformer n'a pas de modèle direct pour la génération de texte comme BigBird-PEGASUS, donc ici, nous utilisons un modèle Longformer pour classification de séquences.\n",
        "# Si vous voulez adapter Longformer pour la génération de texte, vous devrez peut-être fine-tuner un modèle approprié sur une tâche comme la génération.\n",
        "\n",
        "model = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\",\n",
        "                                                           return_dict=True,\n",
        "                                                           torch_dtype=torch.float16,\n",
        "                                                           device_map=\"auto\")\n",
        "\n",
        "# Si vous avez besoin de fine-tuner ce modèle sur une tâche spécifique (comme le résumé), vous devrez remplacer le modèle par un modèle adapté à la génération."
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T00:27:21.753456Z",
          "iopub.execute_input": "2024-12-28T00:27:21.753806Z",
          "iopub.status.idle": "2024-12-28T00:27:47.301507Z",
          "shell.execute_reply.started": "2024-12-28T00:27:21.753766Z",
          "shell.execute_reply": "2024-12-28T00:27:47.300811Z"
        },
        "id": "BidYPSuAla0u",
        "outputId": "ea38de05-7a63-4dc9-c6db-152a7488c1b9",
        "colab": {
          "referenced_widgets": [
            "7884b820e0ac45eea8c922d91f1bdb81",
            "cdb5cb1e1ad0448089133b1f6d48a289",
            "fa5b26bbb7f54eb9ad0bbd3bd6a5d7d6",
            "5523a4292fe14386b7b05e828ac68811",
            "a8568c61090a475f91f71c0b31b2f0ac"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7884b820e0ac45eea8c922d91f1bdb81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdb5cb1e1ad0448089133b1f6d48a289"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa5b26bbb7f54eb9ad0bbd3bd6a5d7d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5523a4292fe14386b7b05e828ac68811"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8568c61090a475f91f71c0b31b2f0ac"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Création du nouveau modèle \"cousu main\" à partir de BART et LONGFORMER"
      ],
      "metadata": {
        "id": "PxUGqU2vla0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On est obligé de faire un peu de \"cuisine\" pour associer les 2 modèles en un seul : par exemple, les sorties de longformer n'ont pas la même taille que les entrées de bart. Il nous faut donc insérer dans la classe une \"couche de projection\".\n",
        "Il faut également dupliquer leurs poids, pour éviter les (mauvais...) mélanges entre eux par la suite.\n",
        "\n",
        "On ne peut pas utiliser la tokenisation de l'un des deux modèles non plus : il nous faut en créer une, tout aussi hybride que notre modèle \"fait main\" à partir de la couture de ces deux modèles issus de hugging face."
      ],
      "metadata": {
        "id": "X0-Vf8nOla0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerTokenizer, BartTokenizer, BartForConditionalGeneration, LongformerModel, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Charger le tokenizer et le modèle Longformer pour l'encodage\n",
        "longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "longformer_model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
        "\n",
        "# Charger le tokenizer et le modèle BART-CNN pour la génération\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Charger un jeu de données pour la tâche de résumé\n",
        "#dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")  # Exemples de dataset pour le résumé\n",
        "\n",
        "# Tokenisation des entrées et des sorties\n",
        "def tokenize_function(examples):\n",
        "    # Tokeniser les articles avec Longformer\n",
        "    model_inputs = longformer_tokenizer(\n",
        "        examples[\"article\"],\n",
        "        #max_length=4096, #trop long pour le GPU de kaggle...\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # Assurer le padding\n",
        "        return_tensors=\"pt\"  # Retourner des tensors PyTorch\n",
        "    )\n",
        "\n",
        "    # Tokeniser les résumés (labels) avec BART tokenizer\n",
        "    labels = bart_tokenizer(\n",
        "        #examples[\"highlights\"],\n",
        "        examples[\"abstract\"],\n",
        "        #max_length=200,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # Assurer le padding\n",
        "        return_tensors=\"pt\"  # Retourner des tensors PyTorch\n",
        "    )\n",
        "\n",
        "    # Ajouter les labels au dictionnaire de sortie\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Appliquer la tokenisation\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Diviser l'ensemble de données en train et validation\n",
        "dataset_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
        "\n",
        "# Extraire les ensembles d'entraînement et de validation\n",
        "train_dataset = dataset_split['train']\n",
        "val_dataset = dataset_split['test']\n",
        "\n",
        "\n",
        "\n",
        "# Définir un modèle combiné qui utilise Longformer comme encodeur et BART pour la génération\n",
        "class LongformerBart(nn.Module):\n",
        "    def __init__(self, longformer_model, bart_model):\n",
        "        super(LongformerBart, self).__init__()\n",
        "        self.longformer = longformer_model\n",
        "        self.bart = bart_model\n",
        "\n",
        " # Ajouter une couche linéaire pour ajuster la dimension des sorties de Longformer\n",
        "        self.longformer_projection = nn.Linear(768, 1024)  # Projeter de 768 à 1024\n",
        "\n",
        " # Dupliquer les poids partagés de BART et Longformer\n",
        "        with torch.no_grad():\n",
        "            # Dupliquer les poids partagés de BART\n",
        "            bart_model.model.shared = torch.nn.Embedding.from_pretrained(bart_model.model.shared.weight.clone())\n",
        "            bart_model.model.encoder.embed_tokens = torch.nn.Embedding.from_pretrained(bart_model.model.encoder.embed_tokens.weight.clone())\n",
        "            bart_model.model.decoder.embed_tokens = torch.nn.Embedding.from_pretrained(bart_model.model.decoder.embed_tokens.weight.clone())\n",
        "\n",
        "            # Dupliquer les poids partagés de Longformer\n",
        "            longformer_model.embeddings.word_embeddings = torch.nn.Embedding.from_pretrained(longformer_model.embeddings.word_embeddings.weight.clone())\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids=None, labels=None):\n",
        "        # Encoder la séquence avec Longformer\n",
        "        encoder_outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
        "\n",
        "  # Appliquer la projection pour ajuster la dimension\n",
        "        encoder_hidden_states = self.longformer_projection(encoder_hidden_states)\n",
        "\n",
        "        # Passer les représentations encodées à BART pour la génération\n",
        "        decoder_outputs = self.bart(\n",
        "            input_ids=decoder_input_ids,\n",
        "            labels=labels,\n",
        "            encoder_outputs=encoder_hidden_states\n",
        "        )\n",
        "\n",
        "        return decoder_outputs\n",
        "\n",
        "# Initialiser le modèle combiné\n",
        "model = LongformerBart(longformer_model, bart_model)\n",
        "\n",
        "# Définir les arguments d'entraînement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./longformer_bart_finetuned\",  # Répertoire de sortie pour enregistrer les résultats\n",
        "    evaluation_strategy=\"epoch\",  # Évaluer après chaque époque\n",
        "    learning_rate=2e-5,  # Taux d'apprentissage\n",
        "    per_device_train_batch_size=2,  # Taille du batch pour l'entraînement\n",
        "    per_device_eval_batch_size=2,   # Taille du batch pour l'évaluation\n",
        "    num_train_epochs=3,  # Nombre d'époques d'entraînement\n",
        "    save_steps=500,  # Sauvegarder moins souvent\n",
        "    weight_decay=0.01,  # L2 regularization\n",
        "    logging_dir=\"./logs\",  # Répertoire pour les logs\n",
        "    logging_steps=10,  # Fréquence des logs\n",
        "    report_to=\"tensorboard\",  # Optionnel : Utiliser TensorBoard pour la visualisation des métriques\n",
        ")\n",
        "\n",
        "\n",
        "# Créer un objet Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,  # Le modèle combiné Longformer+BART\n",
        "    args=training_args,  # Les arguments d'entraînement\n",
        "    train_dataset=train_dataset,  # Jeu d'entraînement\n",
        "    eval_dataset=val_dataset,  # Jeu de validation\n",
        "    tokenizer=longformer_tokenizer,  # Tokenizer pour le pré-traitement des données\n",
        ")\n",
        "\n",
        "# Lancer l'entraînement\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T00:30:08.940781Z",
          "iopub.execute_input": "2024-12-28T00:30:08.941535Z",
          "iopub.status.idle": "2024-12-28T00:40:27.780301Z",
          "shell.execute_reply.started": "2024-12-28T00:30:08.941501Z",
          "shell.execute_reply": "2024-12-28T00:40:27.779440Z"
        },
        "id": "ErHnPpsUla0v",
        "outputId": "85f0a7d8-c8aa-4d7c-aee2-b9ea5f69abcc",
        "colab": {
          "referenced_widgets": [
            "15cba45261994c11a1dca8e0fc38f3be",
            "541843278cef48cea7a00acd750265d8",
            "075a8d822f4b4ea2b4f8def9d234dcd3",
            "df5b6298b21946f596f624ea379c043c",
            "4733cc57de6a40459792bf4ddc1304ad",
            "4b156fdcb2f44a9482e945ec07188582",
            "c8d97cf35a504803bed1c85c49abf486"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15cba45261994c11a1dca8e0fc38f3be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "541843278cef48cea7a00acd750265d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "075a8d822f4b4ea2b4f8def9d234dcd3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df5b6298b21946f596f624ea379c043c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4733cc57de6a40459792bf4ddc1304ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b156fdcb2f44a9482e945ec07188582"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/402 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8d97cf35a504803bed1c85c49abf486"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Train dataset size: 321\nVal dataset size: 81\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='483' max='483' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [483/483 09:56, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.052100</td>\n      <td>0.226625</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.032800</td>\n      <td>6.360270</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.029600</td>\n      <td>7.589591</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=483, training_loss=0.5209710441884541, metrics={'train_runtime': 598.0181, 'train_samples_per_second': 1.61, 'train_steps_per_second': 0.808, 'total_flos': 0.0, 'train_loss': 0.5209710441884541, 'epoch': 3.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Dupliquer les poids partagés pour éviter la duplication de mémoire\n",
        "# Cela doit être fait après l'entraînement et avant la sauvegarde\n",
        "\n",
        "import safetensors.torch as st\n",
        "\n",
        "# Pour Longformer - dupliquer les embeddings\n",
        "with torch.no_grad():  # Empêche la modification des gradients\n",
        "    new_word_embeddings = model.longformer.embeddings.word_embeddings.weight.clone()\n",
        "    model.longformer.embeddings.word_embeddings = torch.nn.Embedding.from_pretrained(new_word_embeddings)\n",
        "\n",
        "# Pour BART - dupliquer les embeddings de l'encodeur et du décodeur\n",
        "with torch.no_grad():  # Empêche la modification des gradients\n",
        "    new_shared_weight = model.bart.model.shared.weight.clone()\n",
        "    model.bart.model.shared = torch.nn.Embedding.from_pretrained(new_shared_weight)\n",
        "\n",
        "    new_encoder_embed = model.bart.model.encoder.embed_tokens.weight.clone()\n",
        "    model.bart.model.encoder.embed_tokens = torch.nn.Embedding.from_pretrained(new_encoder_embed)\n",
        "\n",
        "    new_decoder_embed = model.bart.model.decoder.embed_tokens.weight.clone()\n",
        "    model.bart.model.decoder.embed_tokens = torch.nn.Embedding.from_pretrained(new_decoder_embed)\n",
        "\n",
        "# Sauvegarder les poids dans un fichier classique\n",
        "torch.save(model.state_dict(), 'model_weights.pth')\n",
        "\n",
        "# Charger manuellement avec safetensors (si vous avez configuré safetensors)\n",
        "import safetensors.torch as st\n",
        "\n",
        "# Sauvegarder avec safetensors après avoir dupliqué les poids\n",
        "st.save_file(model.state_dict(), 'model_weights.safetensors')\n",
        "\n",
        "# Sauvegarder le modèle fine-tuné\n",
        "trainer.save_model(\"./longformer_bart_finetuned\")\n",
        "\"\"\"\n",
        "# Sauvegarde du modèle\n",
        "torch.save(longformer_bart_model.state_dict(), \"longformer_bart_finetuned.pth\")\n",
        "\n",
        "# Sauvegarde de la configuration\n",
        "longformer_bart_model.config.to_json_file(\"longformer_bart_config.json\")\n",
        "\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "id": "ik7qS8SXla0v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sauvegarde de ce modèle ainsi créé et fine-tuné"
      ],
      "metadata": {
        "id": "pzK8vkd2la0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On zipe le dossier du modèle ainsi créé et entraîné, pour pouvoir le sauvegarder et le réutiliser ensuite sur notre ensemble de test :"
      ],
      "metadata": {
        "id": "GSJNWv9nla0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir(\"./\"))\n",
        "\n",
        "shutil.make_archive('/kaggle/working/longformer_bart_finetuned', 'zip', './longformer_bart_finetuned')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T01:28:29.159189Z",
          "iopub.execute_input": "2024-12-28T01:28:29.159511Z",
          "iopub.status.idle": "2024-12-28T01:42:54.702503Z",
          "shell.execute_reply.started": "2024-12-28T01:28:29.159485Z",
          "shell.execute_reply": "2024-12-28T01:42:54.701773Z"
        },
        "id": "LcQutPSnla0w",
        "outputId": "ad4d47c0-8f53-4a2f-97d8-305b5d114db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "['config.json', 'longformer_bart_finetuned.pth', 'longformer_bart_finetuned', 'longformer_bart_finetuned_config.json', '.virtual_documents', 'longformer_bart_finetuned.zip', 'longformer_bart_finetuned.safetensors', 'logs']\n",
          "output_type": "stream"
        },
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/kaggle/working/longformer_bart_finetuned.zip'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement de notre modèle fine-tuné et constitué à partir des deux modèles bart et longformer, pour pouvoir l'utiliser pour l'ensemble de test et soumettre nos résumés pour la compétition"
      ],
      "metadata": {
        "id": "4V2jGq8Vla0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Charger le modèle fine tuné afin de l'utiliser\n",
        "import torch\n",
        "from transformers import LongformerModel, BartForConditionalGeneration, LongformerConfig\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Définir la classe LongformerBart\n",
        "class LongformerBart(nn.Module):\n",
        "    def __init__(self, longformer_model, bart_model):\n",
        "        super(LongformerBart, self).__init__()\n",
        "        self.longformer = longformer_model\n",
        "        self.bart = bart_model\n",
        "        self.longformer_projection = nn.Linear(768, 1024)  # Projeter de 768 à 1024\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids=None, labels=None):\n",
        "        encoder_outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
        "        encoder_hidden_states = self.longformer_projection(encoder_hidden_states)  # Appliquer la projection\n",
        "        decoder_outputs = self.bart(input_ids=decoder_input_ids, labels=labels, encoder_outputs=encoder_hidden_states)\n",
        "        return decoder_outputs\n",
        "\n",
        "# Charger Longformer et BART\n",
        "longformer_model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
        "bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Charger les poids de notre modèle fine-tuné\n",
        "model = LongformerBart(longformer_model, bart)\n",
        "model.load_state_dict(torch.load('/kaggle/input/longformer_bart_finetuned/pytorch/default/1/longformer_bart_finetuned/longformer_bart_finetuned.pth'))  # Remplacez par le bon chemin\n",
        "\n",
        "# Charger la configuration\n",
        "# Notez que vous devez avoir la configuration JSON pour le modèle personnalisé\n",
        "config = LongformerConfig.from_json_file('/kaggle/input/longformer_bart_finetuned/pytorch/default/1/longformer_bart_finetuned/longformer_bart_finetuned_config.json')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T16:14:14.564122Z",
          "iopub.execute_input": "2024-12-28T16:14:14.564479Z",
          "iopub.status.idle": "2024-12-28T16:14:49.546188Z",
          "shell.execute_reply.started": "2024-12-28T16:14:14.564449Z",
          "shell.execute_reply": "2024-12-28T16:14:49.545539Z"
        },
        "id": "DwIbM6Adla0x",
        "outputId": "12f7616a-9c2d-451a-c888-ca8054b1e5db",
        "colab": {
          "referenced_widgets": [
            "e303a2a5c790414695a4a5a2893e2ceb",
            "3e0d48fb6f014f92952d78fd1ae4be51",
            "d33c66df7652461fba3e778263319038",
            "7b5eb43a17484ab6bbb02518a034c0b6",
            "cf1165b305344584bb08f962dc6a64e7"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e303a2a5c790414695a4a5a2893e2ceb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e0d48fb6f014f92952d78fd1ae4be51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d33c66df7652461fba3e778263319038"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b5eb43a17484ab6bbb02518a034c0b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf1165b305344584bb08f962dc6a64e7"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "<ipython-input-5-e8c25c7f1ecc>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/input/longformer_bart_finetuned/pytorch/default/1/longformer_bart_finetuned/longformer_bart_finetuned.pth'))  # Remplacez par le bon chemin\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mise en forme des données \"OBS\" de test pour le test du modèle."
      ],
      "metadata": {
        "id": "XgsQq6hNla0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Définir le chemin du dossier contenant les articles\n",
        "dossier_articles_test = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/test/OBS_test/articles_OBS_test\"\n",
        "\n",
        "# Liste des fichiers dans le dossier\n",
        "fichiers_articles = [f for f in os.listdir(dossier_articles_test) if f.startswith(\"article-\")]\n",
        "\n",
        "# Dictionnaire pour stocker les articles par identifiant (avec contenu des fichiers)\n",
        "articles_test = {}\n",
        "\n",
        "# Remplir le dictionnaire avec les fichiers en fonction des identifiants\n",
        "for fichier in fichiers_articles:\n",
        "    # Extraire l'identifiant du fichier article\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "\n",
        "    # Lire le contenu du fichier\n",
        "    with open(os.path.join(dossier_articles_test, fichier), 'r') as f:\n",
        "        contenu_article = f.read()\n",
        "\n",
        "    # Ajouter l'article et son identifiant au dictionnaire\n",
        "    articles_test[identifiant] = {\"article\": contenu_article}\n",
        "\n",
        "# Convertir le dictionnaire en DataFrame\n",
        "df_articles_test = pd.DataFrame.from_dict(articles_test, orient='index')\n",
        "\n",
        "# Réinitialiser l'index pour que 'identifiant' devienne une colonne normale\n",
        "df_articles_test.reset_index(inplace=True)\n",
        "\n",
        "# Renommer les colonnes\n",
        "df_articles_test.columns = ['identifiant', 'article']\n",
        "\n",
        "# Vérifier le DataFrame\n",
        "print(df_articles_test.head())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T16:44:09.636698Z",
          "iopub.execute_input": "2024-12-28T16:44:09.637002Z",
          "iopub.status.idle": "2024-12-28T16:44:09.675130Z",
          "shell.execute_reply.started": "2024-12-28T16:44:09.636978Z",
          "shell.execute_reply": "2024-12-28T16:44:09.674312Z"
        },
        "id": "ODhgpuc_la0y",
        "outputId": "8ff7b7f8-d25c-49a6-a8d8-1e24f5b1fd32"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "  identifiant                                            article\n0    28278130  Cardiometabolic Risk Factors Among 1.3 Million...\n1    34555924  Prostate Cancer Screening and Incidence among ...\n2    35157313  The associated burden of mental health conditi...\n3    36906849  Implementing digital systems to facilitate gen...\n4    37226713  Patient-reported treatment response in chronic...\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisation des données et génération des résumés pour les articles de test de type \"OBS\""
      ],
      "metadata": {
        "id": "N9AuRBI4la0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerTokenizer, BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Initialiser les tokenizers Longformer et BART\n",
        "longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Charger un modèle BART pour la génération de résumé\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenisation des articles et des résumés avec Longformer et BART respectivement.\n",
        "    \"\"\"\n",
        "    # Si 'examples' est une chaîne de texte, placez-la sous une clé dans un dictionnaire\n",
        "    if isinstance(examples, str):\n",
        "        examples = {\"article\": examples}\n",
        "\n",
        "    # Tokenisation de l'article avec Longformer\n",
        "    model_inputs = longformer_tokenizer(\n",
        "        examples[\"article\"],\n",
        "        max_length=1024,  # Longformer est capable de gérer de longs textes\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # Assurer le padding\n",
        "        return_tensors=\"pt\"  # Retourner des tensors PyTorch\n",
        "    )\n",
        "\n",
        "    # Tokenisation des résumés avec BART\n",
        "    labels = bart_tokenizer(\n",
        "        examples[\"abstract\"],  # Assume que \"abstract\" est la cible à résumer\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # Assurer le padding\n",
        "        return_tensors=\"pt\"  # Retourner des tensors PyTorch\n",
        "    )\n",
        "\n",
        "    # Retourne les entrées et labels sous forme de dictionnaire\n",
        "    return {**model_inputs, \"labels\": labels[\"input_ids\"]}\n",
        "\n",
        "def generer_resume(article, model, tokenizer, max_length=512):\n",
        "    \"\"\"\n",
        "    Fonction pour générer un résumé à partir d'un article scientifique donné avec tokenisation.\n",
        "    \"\"\"\n",
        "    # Tokeniser l'article : découpe le texte en tokens et convertit en ids\n",
        "    inputs = tokenizer(\"summarize: \" + article, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
        "\n",
        "    # Générer le résumé\n",
        "    with torch.no_grad():\n",
        "        summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Décoder les IDs générés pour obtenir le résumé en texte\n",
        "    resume = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return resume\n",
        "\n",
        "def traiter_ensemble_test(dataset_RCT, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Fonction qui traite l'ensemble du dataset_RCT et génère des résumés pour chaque article.\n",
        "    \"\"\"\n",
        "    # Convertir le dataset en DataFrame si ce n'est pas déjà fait\n",
        "    df = pd.DataFrame(dataset_RCT)\n",
        "\n",
        "    # Créer une nouvelle colonne pour les résumés générés\n",
        "    df['resume'] = df['article'].apply(lambda x: generer_resume(x, model, tokenizer))\n",
        "\n",
        "    # Afficher le résultat ou sauvegarder les résultats dans un fichier\n",
        "    return df[['identifiant', 'article', 'resume']]\n",
        "\n",
        "\n",
        "# Traiter l'ensemble de test et générer les résumés\n",
        "resultats = traiter_ensemble_test(df_articles_test, model, bart_tokenizer)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(resultats)\n",
        "\n",
        "# Optionnel : Sauvegarder les résultats dans un fichier CSV\n",
        "# resultats.to_csv(\"resultats_resumes.csv\", index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ft0K_TwHla0y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Essai du modèle Bart-large-cnn seul"
      ],
      "metadata": {
        "id": "V3SqhVqDla00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BART est un modèle de transformer encodeur-décodeur (seq2seq : \"sequence to séquence\") avec un encodeur bidirectionnel (similaire à BERT) et un décodeur autoregressif (similaire à GPT).\n",
        "Il est pré-entraîné en masquant des parties du texte et en apprenant à prédire ces parties - comme si on lui demandait de débruiter un texte bruité.\n",
        "Il utilise une attention dense classique pour un transformer, où chaque token de l'entrée prend en compte tous les autres tokens de la séquence. Cela le handicape donc pour nos articles longs (comme tous les modèles à attention dense classique)."
      ],
      "metadata": {
        "id": "_ogvtW3usO7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A vous de le mettre ici, de l'entrainer et de le tester ensuite."
      ],
      "metadata": {
        "id": "_Jxq1tZ3la00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Les trucs ci-dessous sont des restes du chantier, qui ne seront probablement pas utiles en fait (je les garde pour l'instant, \"au cas où\")."
      ],
      "metadata": {
        "id": "J_h5Ti2mla00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parce qu'en fait on est sensés soumettre nos résumés générés sur l'ensemble de test dans la compétition (on a droit à plusieurs soumissions), et c'est la compétition qui va nous renvoyer le score rouge de nos résumés (je n'ai pas encore fait et je ne sais donc pas comment ça marche pour l'instant - je vais bientôt essayer)."
      ],
      "metadata": {
        "id": "n5A5kbqTla00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "f4Y4c_G2la01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install rouge_score\n",
        "from transformers import LongformerTokenizer, BartTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "import os\n",
        "\n",
        "# Initialiser les tokenizers Longformer et BART\n",
        "Longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Si 'examples' est une chaîne de texte, placez-la sous une clé dans un dictionnaire\n",
        "    if isinstance(examples, str):\n",
        "        examples = {\"article\": examples}\n",
        "\n",
        "    # Tokeniser les articles avec Longformer\n",
        "    model_inputs = Longformer_tokenizer(\n",
        "        examples[\"article\"],\n",
        "        #max_length=4096, #trop long pour le GPU de kaggle...\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # Assurer le padding\n",
        "        return_tensors=\"pt\"  # Retourner des tensors PyTorch\n",
        "    )\n",
        "    labels = bart_tokenizer(\n",
        "        #examples[\"highlights\"],\n",
        "        examples[\"abstract\"],\n",
        "        #max_length=200,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # Assurer le padding\n",
        "        return_tensors=\"pt\"  # Retourner des tensors PyTorch\n",
        "    )\n",
        " # Ajouter les labels au dictionnaire de sortie\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "# Chemin vers les articles\n",
        "#articles_path = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/test/OBS_test/articles_OBS_test\"\n",
        "\n",
        "\n",
        "#######################################\n",
        "\n",
        "# Définir les chemins vers les répertoires des articles et des résumés\n",
        "articles_path = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/RCT/articles_RCT\"\n",
        "reference_summaries_path = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/RCT/abstracts_RCT\"\n",
        "\n",
        "\n",
        "\n",
        "###########################################\n",
        "# Initialisation de ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n",
        "\n",
        "# Stocker les scores\n",
        "scores = []\n",
        "\n",
        "# Résumer les 5 premiers fichiers texte\n",
        "for idx, file_name in enumerate(os.listdir(articles_path)):\n",
        "    if idx >= 5:  # Limiter à 5 fichiers\n",
        "        break\n",
        "\n",
        "    file_path = os.path.join(articles_path, file_name)\n",
        "    try:\n",
        "        # Lire le contenu avec encodage explicite\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Erreur d'encodage pour le fichier : {file_name}. Tentative avec 'latin-1'.\")\n",
        "        with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
        "            content = f.read()\n",
        "\n",
        "    # Résumé généré par le modèle\n",
        "    inputs = tokenize_function(content)  # Limiter à 1024 tokens\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=200, num_beams=5, early_stopping=True)\n",
        "    generated_summary = tokenize_function.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # Résumé de référence (remplacement de 'article-' par 'abstract-')\n",
        "    reference_file_name = file_name.replace(\"article-\", \"abstract-\")\n",
        "    reference_file_path = os.path.join(reference_summaries_path, reference_file_name)\n",
        "\n",
        "    # Vérifier si le fichier de référence existe\n",
        "    if not os.path.exists(reference_file_path):\n",
        "        print(f\"Fichier de référence manquant pour : {file_name}.\")\n",
        "        continue\n",
        "\n",
        "    with open(reference_file_path, \"r\", encoding=\"utf-8\") as ref_file:\n",
        "        reference_summary = ref_file.read()\n",
        "\n",
        "    # Calcul de ROUGE-2\n",
        "    rouge_score = scorer.score(reference_summary, generated_summary)\n",
        "    scores.append(rouge_score['rouge2'].fmeasure)  # Récupérer le F1-score\n",
        "\n",
        "    print(f\"Résumé pour {file_name} :\")\n",
        "    print(f\"Résumé généré : {generated_summary}\")\n",
        "    print(f\"Score ROUGE-2 : {rouge_score['rouge2'].fmeasure:.4f}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Calcul de la moyenne des scores\n",
        "if scores:\n",
        "    average_rouge2 = sum(scores) / len(scores)\n",
        "    print(f\"Score moyen ROUGE-2 sur les articles : {average_rouge2:.4f}\")\n",
        "else:\n",
        "    print(\"Aucun score ROUGE-2 calculé.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "22ODq3K2la01",
        "outputId": "70bf33c5-a471-47b9-c49b-c1c867893ac6"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-cc9766b53f84>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Lire le contenu de l'article\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0marticle_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Lire le contenu de l'abstract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x96 in position 1736: invalid start byte"
          ],
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0x96 in position 1736: invalid start byte",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#A faire"
      ],
      "metadata": {
        "id": "71BHna2eNf7X",
        "trusted": true,
        "execution": {
          "execution_failed": "2024-12-27T00:34:33.967Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi #afficher des informations sur les GPU NVIDIA disponibles sur la machine"
      ],
      "metadata": {
        "id": "aVhffGQjyN4P",
        "trusted": true,
        "execution": {
          "execution_failed": "2024-12-27T00:34:33.967Z"
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}