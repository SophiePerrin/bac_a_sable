{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 90248,
          "databundleVersionId": 10480827,
          "sourceType": "competition"
        },
        {
          "sourceId": 212098,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 180807,
          "modelId": 203049
        }
      ],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Confection de r√©sum√©s automatiques d'articles scientifiques par un mod√®le de langue pr√©-entra√Æn√© : √©tude et comparaison des performances de deux mod√®les.\n",
        "\n",
        "**Auteur.e.s :** Sophie perrin, Emma, El Zube, Marius, Sylvano, pour le cours *Representation learning for NLP* @ Master 2 MALIA et MIASHS.\n",
        "\n",
        "**Equipe :** les l√©opards"
      ],
      "metadata": {
        "id": "Jm3aCEp_w2w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pr√©paration de l'environnement\n",
        "\n",
        "1. Se connecter ou se cr√©er un compte sur https://huggingface.co/\n",
        "2. Se cr√©er un nouveau token d'acc√®s : https://huggingface.co/settings/tokens\n",
        "3. Enregistrer ce token comme un secret nomm√© `HF_TOKEN` dans Google Colab (icone \"clef\" dans le bandeau vertical)\n",
        "4. Ex√©cuter le code ci-dessous\n",
        "\n",
        "Note importante pour la vitesse de calcul : pour activer l'acc√©l√©ration GPU dans votre notebook Kaggle :\n",
        "\n",
        "    Ouvrez le menu ¬´ Settings ¬ª : Dans votre notebook Kaggle, cliquez sur le menu ¬´ Accelerator ¬ª en haut de l'√©cran, et choisissez un GPU.\n",
        "\n",
        "    Dans ce m√™me menu, v√©rifiez qu'il n'est pas √©crit \"turn off internet\" - sinon cliquer dessus pour r√©tablir l'acc√®s au reste d'internet depuis kaggle. Si vous n'avez ni \"turn off internet\" ni \"turn on internet\" d'√©crit, alors il faut ajouter votre t√©l√©phone et votre photo pour certifier votre identit√© pour le compte, avant de pouvoir acc√©der au reste d'internet depuis kaggle (n√©cessaire pour charger les mod√®les de huggingface et m√™me les packages de python...!).\n",
        "\n"
      ],
      "metadata": {
        "id": "oiCaaZ8YC8hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()"
      ],
      "metadata": {
        "id": "3pp0itMoj78l",
        "outputId": "6fc6780e-9cd4-447f-c06d-f7f325e10549",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T16:11:16.635146Z",
          "iopub.execute_input": "2024-12-28T16:11:16.635399Z",
          "iopub.status.idle": "2024-12-28T16:11:16.921704Z",
          "shell.execute_reply.started": "2024-12-28T16:11:16.635378Z",
          "shell.execute_reply": "2024-12-28T16:11:16.920576Z"
        },
        "colab": {
          "referenced_widgets": [
            "95322fb8fab547d4bd4a03939cf185c2"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle‚Ä¶",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95322fb8fab547d4bd4a03939cf185c2"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "m_2_maliash_resume_darticles_scientifiques_path = kagglehub.competition_download('m-2-maliash-resume-darticles-scientifiques')\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "id": "z-S3zGw4BfUX",
        "outputId": "b91c7e68-5d81-41a5-e693-63e126680a97",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T16:13:47.922763Z",
          "iopub.execute_input": "2024-12-28T16:13:47.923047Z",
          "iopub.status.idle": "2024-12-28T16:13:48.833929Z",
          "shell.execute_reply.started": "2024-12-28T16:13:47.923024Z",
          "shell.execute_reply": "2024-12-28T16:13:48.833191Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Data source import complete.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Pour cr√©er le \"HF_TOKEN\" dans Kaggle, aller dans \"Add-ons\" (dans les menus de ce notebook, juste √† gauche de \"Help\").\n",
        "#Une fois l√†, aller dans \"secrets\", puis √ßa marche √† peu pr√®s comme dans colab : il faut nommer la variable (HF_TOKEN) et\n",
        "#y copier sa valeur.\n",
        "\n",
        "\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T16:13:50.181926Z",
          "iopub.execute_input": "2024-12-28T16:13:50.182216Z",
          "iopub.status.idle": "2024-12-28T16:13:50.358950Z",
          "shell.execute_reply.started": "2024-12-28T16:13:50.182194Z",
          "shell.execute_reply": "2024-12-28T16:13:50.358317Z"
        },
        "id": "nOrgjrKlla0i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pr√©paration des donn√©es d'entra√Ænement pour leur utilisation par le mod√®le"
      ],
      "metadata": {
        "id": "hoSW39T9djGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Donn√©es du fichier OBS"
      ],
      "metadata": {
        "id": "rL3prK3hla0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On cr√©e un dictionnaire qui apparie les articles de l'ensemble d'entra√Ænement (\"fine tuning\" puisque le mod√®le est d√©j√† pr√©-entra√Æn√©) et leurs r√©sum√©s par leur identifiant commun."
      ],
      "metadata": {
        "id": "iSOUl2N1dtAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Chemins dans Kaggle des dossiers contenant les fichiers\n",
        "dossier_abstracts = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/abstracts_OBS/\"\n",
        "dossier_articles = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/articles_OBS/\"\n",
        "\n",
        "\n",
        "# Liste des fichiers dans chaque dossier\n",
        "fichiers_abstracts = [f for f in os.listdir(dossier_abstracts) if f.startswith(\"abstract-\")]\n",
        "fichiers_articles = [f for f in os.listdir(dossier_articles) if f.startswith(\"article-\")]\n",
        "\n",
        "# Dictionnaires pour stocker les fichiers par identifiant\n",
        "abstracts = {}\n",
        "articles = {}\n",
        "\n",
        "# Remplir les dictionnaires avec les fichiers en fonction des identifiants\n",
        "for fichier in fichiers_abstracts:\n",
        "    # Extraire l'identifiant du fichier abstract\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "     #fichier.split(\"-\") : La m√©thode split(\"-\") divise le nom du fichier\n",
        "    #en une liste de sous-cha√Ænes en utilisant le caract√®re \"-\" comme s√©parateur.\n",
        "    #Par exemple, si le fichier est \"abstract-123.txt\", fichier.split(\"-\") renverra la liste [\"abstract\", \"123.txt\"].\n",
        "    #fichier.split(\"-\")[1] : En prenant l'√©l√©ment d'indice 1 de cette liste (c'est-√†-dire \"123.txt\"), on obtient la partie du nom du fichier apr√®s le pr√©fixe \"abstract-\".\n",
        "    #split(\".\")[0] : Ensuite, on divise cette cha√Æne \"123.txt\" avec split(\".\"), ce qui donne [\"123\", \"txt\"].\n",
        "    #On prend le premier √©l√©ment de la liste (c'est-√†-dire \"123\") qui est l'identifiant unique de l'abstract.\n",
        "    abstracts[identifiant] = fichier\n",
        "    #Cette ligne ajoute une entr√©e dans le dictionnaire abstracts, o√π la cl√© est : identifiant (par exemple \"123\")\n",
        "    #et la valeur est le nom du fichier : fichier (par exemple \"abstract-123.txt\").\n",
        "\n",
        "for fichier in fichiers_articles:\n",
        "    # Extraire l'identifiant du fichier article\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "    articles[identifiant] = fichier\n",
        "\n",
        "# Dictionnaire pour stocker les appariements\n",
        "appariements = {}\n",
        "\n",
        "# Liste des abstracts et articles non-appari√©s\n",
        "non_apparies_abstracts = []\n",
        "non_apparies_articles = []\n",
        "\n",
        "# Appariement des fichiers abstracts et articles par identifiant\n",
        "for identifiant in abstracts:\n",
        "    if identifiant in articles:\n",
        "        # Ajouter l'appariement au dictionnaire\n",
        "        appariements[identifiant] = {\n",
        "            \"abstract\": abstracts[identifiant],\n",
        "            \"article\": articles[identifiant]\n",
        "        }\n",
        "    else:\n",
        "        # Ajouter √† la liste des abstracts non appari√©s\n",
        "        non_apparies_abstracts.append(abstracts[identifiant])\n",
        "\n",
        "# V√©rifier les articles non-appari√©s\n",
        "for identifiant in articles:\n",
        "    if identifiant not in abstracts:\n",
        "        # Ajouter √† la liste des articles non appari√©s\n",
        "        non_apparies_articles.append(articles[identifiant])\n",
        "\n",
        "# Affichage des appariements\n",
        "\"\"\"\n",
        "print(\"Appariements :\")\n",
        "for identifiant, fichiers in appariements.items():\n",
        "    print(f\"Identifiant {identifiant}:\")\n",
        "    print(f\"  Abstract: {fichiers['abstract']}\")\n",
        "    print(f\"  Article: {fichiers['article']}\")\n",
        "\"\"\"\n",
        "# Affichage des abstracts non-appari√©s\n",
        "print(\"\\nAbstracts non-appari√©s :\")\n",
        "for abstract in non_apparies_abstracts:\n",
        "    print(abstract)\n",
        "\n",
        "# Affichage des articles non-appari√©s\n",
        "print(\"\\nArticles non-appari√©s :\")\n",
        "for article in non_apparies_articles:\n",
        "    print(article)"
      ],
      "metadata": {
        "id": "_5wciJ57jnhk",
        "outputId": "b9096f10-f7a2-4209-fc66-42350f644ec5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T00:27:00.178159Z",
          "iopub.execute_input": "2024-12-28T00:27:00.178442Z",
          "iopub.status.idle": "2024-12-28T00:27:00.213808Z",
          "shell.execute_reply.started": "2024-12-28T00:27:00.178423Z",
          "shell.execute_reply": "2024-12-28T00:27:00.213155Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nAbstracts non-appari√©s :\nabstract-36404343.txt\nabstract-38022520.txt\nabstract-37614109.txt\nabstract-36944082.txt\nabstract-32017677.txt\nabstract-36944050.txt\nabstract-36891751.txt\nabstract-35081022.txt\nabstract-32091358.txt\nabstract-31625835.txt\nabstract-36066965.txt\nabstract-36519326.txt\nabstract-36525381.txt\nabstract-36314570.txt\nabstract-37833688.txt\nabstract-27423055.txt\nabstract-37796016.txt\nabstract-37026745.txt\nabstract-36068298.txt\nabstract-35324507.txt\nabstract-37726286.txt\nabstract-37102598.txt\nabstract-36951168.txt\nabstract-24279685.txt\nabstract-36944043.txt\nabstract-32946618.txt\nabstract-36372692.txt\n\nArticles non-appari√©s :\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "A noter que quelques r√©sum√©s n'ont pas d'article qui leur correspond !"
      ],
      "metadata": {
        "id": "ySTyiFhjd0i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On retravaille un peu la forme de ce dictionnaire pour pouvoir le convertir au format \"dataset\" de Hugging face, n√©cessaire pour pouvoir entra√Æner le mod√®le choisi dessus.\n"
      ],
      "metadata": {
        "id": "7axwPxrAd7Di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets #le \"-U\" demande l'installation de la toute derni√®re version du package\n",
        "from datasets import Dataset #importe la classe Dataset de la biblioth√®que datasets de Hugging Face.\n",
        "#Cette classe est utilis√©e pour manipuler des ensembles de donn√©es dans un format qui peut √™tre utilis√© pour l'entra√Ænement de mod√®les issus d'Hugging Face.\n",
        "\n",
        "# Limiter le nombre de threads de JAX √† 1 (pour √©viter les probl√®mes dans kaggle)\n",
        "os.environ[\"JAX_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "#Notre dictionnaire appariements n'est pas tout √† fait sous la bonne forme pour √™tre transform√© en dataset de Hugging Face :\n",
        "#on le remanie donc pour qu'il ait cette bonne forme.\n",
        "\n",
        "# Initialisation des listes vides\n",
        "identifiants = []\n",
        "articles = []\n",
        "abstracts = []\n",
        "\n",
        "# Remplir les listes avec les donn√©es extraites du dictionnaire appariements\n",
        "for identifiant, values in appariements.items():\n",
        "    identifiants.append(identifiant)         # Ajout de l'identifiant\n",
        "    articles.append(values[\"article\"])      # Ajout de l'article\n",
        "    abstracts.append(values[\"abstract\"])    # Ajout de l'abstract\n",
        "\n",
        "# Cr√©er un dictionnaire avec les listes\n",
        "data = {\n",
        "    \"identifiant\": identifiants,\n",
        "    \"article\": articles,\n",
        "    \"abstract\": abstracts\n",
        "}\n",
        "\n",
        "# Cr√©er le Dataset en convertissant notre nouveau dictionnaire via l'instruction Dataset.from_dict() de Dataset\n",
        "dataset = Dataset.from_dict(data)\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "wJ-fvqMz7ahj",
        "outputId": "5cc78080-3b1a-48ba-9db2-27872f4ec33c",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T00:27:05.066278Z",
          "iopub.execute_input": "2024-12-28T00:27:05.066565Z",
          "iopub.status.idle": "2024-12-28T00:27:10.908628Z",
          "shell.execute_reply.started": "2024-12-28T00:27:05.066544Z",
          "shell.execute_reply": "2024-12-28T00:27:10.907796Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDataset({\n    features: ['identifiant', 'article', 'abstract'],\n    num_rows: 402\n})\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## le fichier √† utiliser pour l'entra√Ænement du mod√®le sur les articles de type OBS sera donc \"dataset\""
      ],
      "metadata": {
        "id": "0oFbiBnWla0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Donn√©es RCT"
      ],
      "metadata": {
        "id": "O1JUk4V6la0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On proc√®de de m√™me pour les donn√©es des articles de type \"RCT\""
      ],
      "metadata": {
        "id": "pfykvJwpla0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Chemins dans Kaggle des dossiers contenant les fichiers\n",
        "dossier_abstracts = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/RCT/abstracts_RCT/\"\n",
        "dossier_articles = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/RCT/articles_RCT/\"\n",
        "\n",
        "\n",
        "# Liste des fichiers dans chaque dossier\n",
        "fichiers_abstracts = [f for f in os.listdir(dossier_abstracts) if f.startswith(\"abstract-\")]\n",
        "fichiers_articles = [f for f in os.listdir(dossier_articles) if f.startswith(\"article-\")]\n",
        "\n",
        "# Dictionnaires pour stocker les fichiers par identifiant\n",
        "abstracts = {}\n",
        "articles = {}\n",
        "\n",
        "# Remplir les dictionnaires avec les fichiers en fonction des identifiants\n",
        "for fichier in fichiers_abstracts:\n",
        "    # Extraire l'identifiant du fichier abstract\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "     #fichier.split(\"-\") : La m√©thode split(\"-\") divise le nom du fichier\n",
        "    #en une liste de sous-cha√Ænes en utilisant le caract√®re \"-\" comme s√©parateur.\n",
        "    #Par exemple, si le fichier est \"abstract-123.txt\", fichier.split(\"-\") renverra la liste [\"abstract\", \"123.txt\"].\n",
        "    #fichier.split(\"-\")[1] : En prenant l'√©l√©ment d'indice 1 de cette liste (c'est-√†-dire \"123.txt\"), on obtient la partie du nom du fichier apr√®s le pr√©fixe \"abstract-\".\n",
        "    #split(\".\")[0] : Ensuite, on divise cette cha√Æne \"123.txt\" avec split(\".\"), ce qui donne [\"123\", \"txt\"].\n",
        "    #On prend le premier √©l√©ment de la liste (c'est-√†-dire \"123\") qui est l'identifiant unique de l'abstract.\n",
        "    abstracts[identifiant] = fichier\n",
        "    #Cette ligne ajoute une entr√©e dans le dictionnaire abstracts, o√π la cl√© est : identifiant (par exemple \"123\")\n",
        "    #et la valeur est le nom du fichier : fichier (par exemple \"abstract-123.txt\").\n",
        "\n",
        "for fichier in fichiers_articles:\n",
        "    # Extraire l'identifiant du fichier article\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "    articles[identifiant] = fichier\n",
        "\n",
        "# Dictionnaire pour stocker les appariements\n",
        "appariements_RCT = {}\n",
        "\n",
        "# Liste des abstracts et articles non-appari√©s\n",
        "non_apparies_abstracts_RCT = []\n",
        "non_apparies_articles_RCT = []\n",
        "\n",
        "# Appariement des fichiers abstracts et articles par identifiant\n",
        "for identifiant in abstracts:\n",
        "    if identifiant in articles:\n",
        "        # Ajouter l'appariement au dictionnaire\n",
        "        appariements_RCT[identifiant] = {\n",
        "            \"abstract\": abstracts[identifiant],\n",
        "            \"article\": articles[identifiant]\n",
        "        }\n",
        "    else:\n",
        "        # Ajouter √† la liste des abstracts non appari√©s\n",
        "        non_apparies_abstracts_RCT.append(abstracts[identifiant])\n",
        "\n",
        "# V√©rifier les articles non-appari√©s\n",
        "for identifiant in articles:\n",
        "    if identifiant not in abstracts:\n",
        "        # Ajouter √† la liste des articles non appari√©s\n",
        "        non_apparies_articles_RCT.append(articles[identifiant])\n",
        "\n",
        "# Affichage des appariements\n",
        "\"\"\"\n",
        "print(\"Appariements_RCT :\")\n",
        "for identifiant, fichiers in appariements_RCT.items():\n",
        "    print(f\"Identifiant {identifiant}:\")\n",
        "    print(f\"  Abstract: {fichiers['abstract']}\")\n",
        "    print(f\"  Article: {fichiers['article']}\")\n",
        "\"\"\"\n",
        "# Affichage des abstracts non-appari√©s\n",
        "print(\"\\nAbstracts non-appari√©s :\")\n",
        "for abstract in non_apparies_abstracts_RCT:\n",
        "    print(abstract)\n",
        "\n",
        "# Affichage des articles non-appari√©s\n",
        "print(\"\\nArticles non-appari√©s :\")\n",
        "for article in non_apparies_articles_RCT:\n",
        "    print(article)"
      ],
      "metadata": {
        "trusted": true,
        "id": "i9zDXRwyla0o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets #le \"-U\" demande l'installation de la toute derni√®re version du package\n",
        "from datasets import Dataset #importe la classe Dataset de la biblioth√®que datasets de Hugging Face.\n",
        "#Cette classe est utilis√©e pour manipuler des ensembles de donn√©es dans un format qui peut √™tre utilis√© pour l'entra√Ænement de mod√®les issus d'Hugging Face.\n",
        "\n",
        "# Limiter le nombre de threads de JAX √† 1 (pour √©viter les probl√®mes dans kaggle)\n",
        "os.environ[\"JAX_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "#Notre dictionnaire appariements_RCT n'est pas tout √† fait sous la bonne forme pour √™tre transform√© en dataset de Hugging Face :\n",
        "#on le remanie donc pour qu'il ait cette bonne forme.\n",
        "\n",
        "# Initialisation des listes vides\n",
        "identifiants = []\n",
        "articles = []\n",
        "abstracts = []\n",
        "\n",
        "# Remplir les listes avec les donn√©es extraites du dictionnaire appariements\n",
        "for identifiant, values in appariements_RCT.items():\n",
        "    identifiants.append(identifiant)         # Ajout de l'identifiant\n",
        "    articles.append(values[\"article\"])      # Ajout de l'article\n",
        "    abstracts.append(values[\"abstract\"])    # Ajout de l'abstract\n",
        "\n",
        "# Cr√©er un dictionnaire avec les listes\n",
        "data = {\n",
        "    \"identifiant\": identifiants,\n",
        "    \"article\": articles,\n",
        "    \"abstract\": abstracts\n",
        "}\n",
        "\n",
        "# Cr√©er le Dataset en convertissant notre nouveau dictionnaire via l'instruction Dataset.from_dict() de Dataset\n",
        "dataset_RCT = Dataset.from_dict(data)\n",
        "\n",
        "#print(dataset_RCT)"
      ],
      "metadata": {
        "trusted": true,
        "id": "9foOVsZwla0o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Le fichier √† utiliser pour l'entrainement sur les donn√©es RCT sera donc \"dataset_RCT\""
      ],
      "metadata": {
        "id": "2qTVuUuKla0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Essai du mod√®le bigbird-pegasus-large-arxiv"
      ],
      "metadata": {
        "id": "Z_GMNlNula0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce mod√®le a pour singularit√©, par rapport √† des mod√®les de type BERT, d'utiliser une version modifi√©e de l'architecture Transformer : il a un m√©canisme d'attention sparse (attention r√©duite) par blocs. Concr√®tement, cela signifie qu'au lieu de calculer l'attention entre toutes les positions du texte, il divise la s√©quence (le texte) en blocs et applique l'attention uniquement au sein de ces blocs, ainsi qu'entre certains blocs (par exemple les blocs voisins ou un √©chantillonnage al√©atoire de blocs).\n",
        "\n",
        "Cela permet d'√™tre √©conome en temps de calcul : avec un mod√®le de type BERT, ce dernier croissait en $O(N^2)$, o√π $N$ repr√©sente la taille du texte (ou document). Avec le m√©canisme d'attention sparse par blocs, on tombe √† des temps de calcul en $O(N.log(N))$ ou en $O(N)$.\n",
        "\n",
        "Pour nos articles scientifiques, qui sont des documents longs, ce type de mod√®les, avec attention sparse par blocs, peut donc √™tre particuli√®rement pertinent."
      ],
      "metadata": {
        "id": "fUiZk0D9q-c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On convertit maintenant le texte des articles et des r√©sum√©s (ici ceux de type \"OBS\") en s√©quences de tokens pour le mod√®le bigbird-pegasus-large-arxiv."
      ],
      "metadata": {
        "id": "eDcpedpneMHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour pr√©parer les donn√©es (tokenisation) pour un mod√®le charg√© depuis huggingface\n",
        "def tokenize_function(examples):\n",
        "    # Tokeniser les articles et les r√©sum√©s\n",
        "    inputs = tokenizer(examples['article'], truncation=True, padding=\"max_length\", max_length=1024) #donn√©es en entr√©e\n",
        "    #examples['article'] : C'est la donn√©e brute (l'article) fournie √† la fonction sous forme de texte.\n",
        "    #Cette donn√©e vient d'un Dataset ou d'une DataLoader et est pass√©e sous forme de liste de textes (articles).\n",
        "    #tokenizer(examples['article']) : Cette ligne utilise le tokenizer pour convertir le texte brut des articles en une s√©quence de tokens.\n",
        "    #Le tokenizer transforme le texte en un format compr√©hensible par le mod√®le (i.e., des IDs de tokens).\n",
        "\n",
        "    #truncation=True : Si le texte est plus long que la longueur maximale d√©finie, il sera tronqu√© pour correspondre √† cette longueur maximale.\n",
        "\n",
        "    #padding=\"max_length\" : Cela permet de remplir (padd) le texte pour qu'il atteigne la longueur maximale sp√©cifi√©e.\n",
        "    #Si un article est plus court que la longueur maximale, des tokens de remplissage seront ajout√©s.\n",
        "\n",
        "    #max_length=1024 : La longueur maximale des s√©quences d'entr√©e est fix√©e √† 1024 tokens. Si un article est plus long que cela, il sera tronqu√© √† 1024 tokens.\n",
        "\n",
        "    targets = tokenizer(examples['abstract'], truncation=True, padding=\"max_length\", max_length=256)#donn√©es cible de l'entrainement du mod√®le\n",
        "\n",
        "    # Retourner les inputs et targets sous la forme de dictionnaires\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    #inputs[\"labels\"] : Dans un mod√®le de g√©n√©ration de texte comme BigBirdPegasus, les labels sont les s√©quences que le mod√®le doit pr√©dire\n",
        "    #(les r√©sum√©s dans ce cas). En d'autres termes, le mod√®le apprend √† pr√©dire les tokens du r√©sum√© √† partir des tokens de l'article.\n",
        "    #targets[\"input_ids\"] : input_ids est la repr√©sentation des tokens du r√©sum√© (les IDs num√©riques des tokens). Ces tokens sont utilis√©s comme labels lors de l'entra√Ænement,\n",
        "    #c'est-√†-dire que le mod√®le essaiera de pr√©dire ces input_ids lorsqu'il verra les tokens des articles.\n",
        "    #Cette ligne ajoute donc les input_ids des r√©sum√©s dans la cl√© \"labels\" des donn√©es d'entr√©e, qui est utilis√©e pendant l'entra√Ænement pour calculer la perte\n",
        "    #entre les pr√©dictions du mod√®le et les r√©sum√©s r√©els.\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "41fMtp6-pHgH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-27T20:31:18.348955Z",
          "iopub.execute_input": "2024-12-27T20:31:18.349239Z",
          "iopub.status.idle": "2024-12-27T20:31:18.353920Z",
          "shell.execute_reply.started": "2024-12-27T20:31:18.349219Z",
          "shell.execute_reply": "2024-12-27T20:31:18.353031Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement du mod√®le bigbird-pegasus-large-arxiv\n",
        "\n"
      ],
      "metadata": {
        "id": "TxbTu-oNDOC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer # Import TrainingArguments and Trainer\n",
        "\n",
        "#Instancier le tokenizer pour le mod√®le pegasus\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/bigbird-pegasus-large-arxiv/\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
        "#chargement du mod√®le :\n",
        "# by default encoder-attention is `block_sparse` with num_random_blocks=3, block_size=64/par d√©faut l'encoder-attention est \"block sparse\"\n",
        "#avec num_random_blocks=3 et block_size=64\n",
        "#model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"/kaggle/input/bigbird-pegasus-large-arxiv/\",\n",
        "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\",\n",
        "                                                               return_dict=True,\n",
        "                                                               torch_dtype=torch.float16,\n",
        "                                                               device_map=\"auto\")\n",
        "\n",
        "######################Options pour modifier la taille et le nombre des blocs d'attention :\n",
        "# decoder attention type can't be changed & will be \"original_full\"\n",
        "# you can change `attention_type` (encoder only) to full attention like this:\n",
        "##model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", attention_type=\"original_full\")\n",
        "\n",
        "# you can change `block_size` & `num_random_blocks` like this:\n",
        "##model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", block_size=16, num_random_blocks=2)\n",
        "######################\n",
        "\n"
      ],
      "metadata": {
        "id": "j1WNnycYXvQM",
        "outputId": "89d58390-0900-4b3b-f348-a00bb02a5e7c",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-27T00:28:45.486060Z",
          "iopub.execute_input": "2024-12-27T00:28:45.486576Z",
          "iopub.status.idle": "2024-12-27T00:29:10.018487Z",
          "shell.execute_reply.started": "2024-12-27T00:28:45.486546Z",
          "shell.execute_reply": "2024-12-27T00:29:10.017849Z"
        },
        "colab": {
          "referenced_widgets": [
            "3139ebd1e216433596ad25ee89cf008f",
            "37c2391201ad4c07bb91b64fafe52e89"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/2.31G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3139ebd1e216433596ad25ee89cf008f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/232 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37c2391201ad4c07bb91b64fafe52e89"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrainement du mod√®le bigbird-pegasus-large-arxiv et conclusions sur ce mod√®le pour notre comp√©tition kaggle"
      ],
      "metadata": {
        "id": "b5aidQKJla0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On param√®tre l'entra√Ænement du mod√®le.\n",
        "\n",
        "Probl√®me : m√™me en modifiant les param√®tres de training_args comme ci-dessous, pour tenter de consommer moins de m√©moire, l'entra√Ænement de ce mod√®le fait planter la session kaggle car il consomme trop de m√©moire - y compris en utilisant les GPU 100, ceux qui comportent le plus de m√©moire sur kaggle.\n",
        "\n",
        "Nous allons donc tester un autre mod√®le, compos√© par l'assemblage de bart-large-cnn et longformer-base-4096, et bart-large-cnn seul (simplement fine-tun√© sur nos donn√©es d'entra√Ænement)."
      ],
      "metadata": {
        "id": "s9i3nuomeifY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Appliquer la tokenisation\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# D√©finir les arguments d'entra√Ænement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bigbird_pegasus_finetuned\",  # R√©pertoire de sortie pour enregistrer les r√©sultats\n",
        "    evaluation_strategy=\"epoch\",  # √âvaluer apr√®s chaque √©poque\n",
        "    learning_rate=2e-5,  # Taux d'apprentissage\n",
        "    per_device_train_batch_size=1, #2 je r√©duis pour √©viter de planter kaggle,  # Taille du batch pour l'entra√Ænement\n",
        "    per_device_eval_batch_size=1, #2 idem,   # Taille du batch pour l'√©valuation\n",
        "    gradient_accumulation_steps=8,  # Accumuler les gradients sur 8 batches avant la mise √† jour (r√©duit l'utilisation de la m√©moire, toujours pour √©viter de planter kaggle)\n",
        "                                    #cela \"simule\" une taille de batch plus grande que ce qu'elle n'est en r√©alit√© ici\n",
        "    fp16=False,  # Activer l'entra√Ænement en 16-bit - toujours pour √©viter de planter kaggle\n",
        "    num_train_epochs=3,  # Nombre d'√©poques d'entra√Ænement\n",
        "    dataloader_num_workers=4,  # Utiliser 4 processus pour charger les donn√©es en parall√®le\n",
        "    gradient_checkpointing=True,  # Activer gradient checkpointing\n",
        "    save_steps=500,  # Sauvegarder moins souvent\n",
        "    weight_decay=0.01,  # L2 regularization\n",
        "    save_total_limit=2,  # Limite du nombre de sauvegardes du mod√®le\n",
        "    logging_dir=\"./logs\",  # R√©pertoire pour les logs\n",
        "    logging_steps=100,  # Fr√©quence des logs\n",
        "    report_to=\"tensorboard\",  # Optionnel : Utiliser TensorBoard pour la visualisation des m√©triques\n",
        ")\n",
        "\n",
        "# Cr√©er un objet Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,  # Le mod√®le √† fine-tuner\n",
        "    args=training_args,  # Les arguments d'entra√Ænement\n",
        "    train_dataset=train_dataset,  # Jeu d'entra√Ænement\n",
        "    eval_dataset=val_dataset,  # Jeu de validation\n",
        "    compute_metrics=compute_metrics,  # Fonction pour calculer les m√©triques - elle nous permettra d'utiliser le score \"ROUGE\"\n",
        ")\n",
        "\n",
        "# Lancer l'entra√Ænement\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "# Sauvegarder le mod√®le fine-tun√©\n",
        "trainer.save_model(\"./bigbird_pegasus_finetuned\")"
      ],
      "metadata": {
        "id": "9_UU-nmJqoiQ",
        "outputId": "08af7291-0207-4cbf-b546-3431e5c94376",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-27T00:29:10.020485Z",
          "iopub.execute_input": "2024-12-27T00:29:10.020922Z",
          "execution_failed": "2024-12-27T00:34:33.966Z"
        },
        "colab": {
          "referenced_widgets": [
            "4f7cb4140108423fa54365a1dcdd7810"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/402 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f7cb4140108423fa54365a1dcdd7810"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='46' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 46/135 04:23 < 08:53, 0.17 it/s, Epoch 1.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='41' max='41' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [41/41 00:08]\n    </div>\n    "
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confection d'un mod√®le compos√© √† partir de longformer et bart"
      ],
      "metadata": {
        "id": "K67A1zjGla0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement du mod√®le  allenai/longformer-base-4096\n",
        "\n"
      ],
      "metadata": {
        "id": "EcBwQBTGla0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le mod√®le bigbird-pegasus-large-arivx demande trop de m√©moire pour notre plateforme de comp√©tition kaggle.\n",
        "\n",
        "Nous devons donc nous orienter vers un mod√®le de compromis, n√©anmoins adapt√© au traitement d'articles scientifiques longs.\n",
        "Il nous faut donc chercher dans la famille des mod√®les √† attention \"sparse\", les seuls vraiment adapt√©s pour ces textes longs.\n",
        "\n",
        "allenai/longformer-large-4096 partage avec bigbird ce m√©canisme d'attention sparse : tout comme lui, il utilise une fen√™tre d'attention \"glissante\". Tout comme lui, il y ajoute une attention √† plus longue port√©e sur les tokens importants, ce qui lui permet d'interagir avec tous les autres tokens de la s√©quence pour capturer des relations √† long terme. Mais en revanche, contrairement √† lui, il n'a pas de m√©canisme d'attention al√©atoire qui se surajoute √† ces deux couches d'attention, et qui lui permet de mieux capter des relations globales...mais consomme de la m√©moire vive, lors de son entra√Ænement (fine tuning) notamment.\n",
        "\n",
        "En revanche, il n'est pas un g√©n√©rateur de texte : il nous faut donc l'associer √† un mod√®le de type GPT ou autre qui permet quant √† lui la g√©n√©ration. Ce, dans le but de bonifier les performances du mod√®le g√©n√©rateur par les bonnes performances de longformer sur les textes longs.\n",
        "On l'associe avec le mod√®le g√©n√©rateur bart-large-cnn, association qu'on cr√©e \"√† la main\" via la confection d'une classe sp√©cifique.\n",
        "\n",
        "Comme le mod√®le ainsi cr√©√© reste trop volumineux, on y remplace finalement allenai/longformer-large-4096 par une version plus l√©g√®re : allenai/longformer-base-4096.\n",
        "\n"
      ],
      "metadata": {
        "id": "6JR1M1Cela0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer # Import TrainingArguments and Trainer\n",
        "\n",
        "# Instancier le tokenizer pour le mod√®le Longformer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "\n",
        "# Chargement du mod√®le Longformer\n",
        "# Longformer n'a pas de mod√®le direct pour la g√©n√©ration de texte comme BigBird-PEGASUS, donc ici, nous utilisons un mod√®le Longformer pour classification de s√©quences.\n",
        "# Si vous voulez adapter Longformer pour la g√©n√©ration de texte, vous devrez peut-√™tre fine-tuner un mod√®le appropri√© sur une t√¢che comme la g√©n√©ration.\n",
        "\n",
        "model = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\",\n",
        "                                                           return_dict=True,\n",
        "                                                           torch_dtype=torch.float16,\n",
        "                                                           device_map=\"auto\")\n",
        "\n",
        "# Si vous avez besoin de fine-tuner ce mod√®le sur une t√¢che sp√©cifique (comme le r√©sum√©), vous devrez remplacer le mod√®le par un mod√®le adapt√© √† la g√©n√©ration."
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T00:27:21.753456Z",
          "iopub.execute_input": "2024-12-28T00:27:21.753806Z",
          "iopub.status.idle": "2024-12-28T00:27:47.301507Z",
          "shell.execute_reply.started": "2024-12-28T00:27:21.753766Z",
          "shell.execute_reply": "2024-12-28T00:27:47.300811Z"
        },
        "id": "BidYPSuAla0u",
        "outputId": "ea38de05-7a63-4dc9-c6db-152a7488c1b9",
        "colab": {
          "referenced_widgets": [
            "7884b820e0ac45eea8c922d91f1bdb81",
            "cdb5cb1e1ad0448089133b1f6d48a289",
            "fa5b26bbb7f54eb9ad0bbd3bd6a5d7d6",
            "5523a4292fe14386b7b05e828ac68811",
            "a8568c61090a475f91f71c0b31b2f0ac"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7884b820e0ac45eea8c922d91f1bdb81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdb5cb1e1ad0448089133b1f6d48a289"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa5b26bbb7f54eb9ad0bbd3bd6a5d7d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5523a4292fe14386b7b05e828ac68811"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8568c61090a475f91f71c0b31b2f0ac"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cr√©ation du nouveau mod√®le \"cousu main\" √† partir de BART et LONGFORMER"
      ],
      "metadata": {
        "id": "PxUGqU2vla0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On est oblig√© de faire un peu de \"cuisine\" pour associer les 2 mod√®les en un seul : par exemple, les sorties de longformer n'ont pas la m√™me taille que les entr√©es de bart. Il nous faut donc ins√©rer dans la classe une \"couche de projection\".\n",
        "Il faut √©galement dupliquer leurs poids, pour √©viter les (mauvais...) m√©langes entre eux par la suite.\n",
        "\n",
        "On ne peut pas utiliser la tokenisation de l'un des deux mod√®les non plus : il nous faut en cr√©er une, tout aussi hybride que notre mod√®le \"fait main\" √† partir de la couture de ces deux mod√®les issus de hugging face."
      ],
      "metadata": {
        "id": "X0-Vf8nOla0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerTokenizer, BartTokenizer, BartForConditionalGeneration, LongformerModel, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Charger le tokenizer et le mod√®le Longformer pour l'encodage\n",
        "longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "longformer_model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
        "\n",
        "# Charger le tokenizer et le mod√®le BART-CNN pour la g√©n√©ration\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Charger un jeu de donn√©es pour la t√¢che de r√©sum√©\n",
        "#dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")  # Exemples de dataset pour le r√©sum√©\n",
        "\n",
        "# Tokenisation des entr√©es et des sorties\n",
        "def tokenize_function(examples):\n",
        "    # Tokeniser les articles avec Longformer\n",
        "    model_inputs = longformer_tokenizer(\n",
        "        examples[\"article\"],\n",
        "        #max_length=4096, #trop long pour le GPU de kaggle...\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # Assurer le padding\n",
        "        return_tensors=\"pt\"  # Retourner des tensors PyTorch\n",
        "    )\n",
        "\n",
        "    # Tokeniser les r√©sum√©s (labels) avec BART tokenizer\n",
        "    labels = bart_tokenizer(\n",
        "        #examples[\"highlights\"],\n",
        "        examples[\"abstract\"],\n",
        "        #max_length=200,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # Assurer le padding\n",
        "        return_tensors=\"pt\"  # Retourner des tensors PyTorch\n",
        "    )\n",
        "\n",
        "    # Ajouter les labels au dictionnaire de sortie\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Appliquer la tokenisation\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Diviser l'ensemble de donn√©es en train et validation\n",
        "dataset_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
        "\n",
        "# Extraire les ensembles d'entra√Ænement et de validation\n",
        "train_dataset = dataset_split['train']\n",
        "val_dataset = dataset_split['test']\n",
        "\n",
        "\n",
        "\n",
        "# D√©finir un mod√®le combin√© qui utilise Longformer comme encodeur et BART pour la g√©n√©ration\n",
        "class LongformerBart(nn.Module):\n",
        "    def __init__(self, longformer_model, bart_model):\n",
        "        super(LongformerBart, self).__init__()\n",
        "        self.longformer = longformer_model\n",
        "        self.bart = bart_model\n",
        "\n",
        " # Ajouter une couche lin√©aire pour ajuster la dimension des sorties de Longformer\n",
        "        self.longformer_projection = nn.Linear(768, 1024)  # Projeter de 768 √† 1024\n",
        "\n",
        " # Dupliquer les poids partag√©s de BART et Longformer\n",
        "        with torch.no_grad():\n",
        "            # Dupliquer les poids partag√©s de BART\n",
        "            bart_model.model.shared = torch.nn.Embedding.from_pretrained(bart_model.model.shared.weight.clone())\n",
        "            bart_model.model.encoder.embed_tokens = torch.nn.Embedding.from_pretrained(bart_model.model.encoder.embed_tokens.weight.clone())\n",
        "            bart_model.model.decoder.embed_tokens = torch.nn.Embedding.from_pretrained(bart_model.model.decoder.embed_tokens.weight.clone())\n",
        "\n",
        "            # Dupliquer les poids partag√©s de Longformer\n",
        "            longformer_model.embeddings.word_embeddings = torch.nn.Embedding.from_pretrained(longformer_model.embeddings.word_embeddings.weight.clone())\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids=None, labels=None):\n",
        "        # Encoder la s√©quence avec Longformer\n",
        "        encoder_outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
        "\n",
        "  # Appliquer la projection pour ajuster la dimension\n",
        "        encoder_hidden_states = self.longformer_projection(encoder_hidden_states)\n",
        "\n",
        "        # Passer les repr√©sentations encod√©es √† BART pour la g√©n√©ration\n",
        "        decoder_outputs = self.bart(\n",
        "            input_ids=decoder_input_ids,\n",
        "            labels=labels,\n",
        "            encoder_outputs=encoder_hidden_states\n",
        "        )\n",
        "\n",
        "        return decoder_outputs\n",
        "\n",
        "# Initialiser le mod√®le combin√©\n",
        "model = LongformerBart(longformer_model, bart_model)\n",
        "\n",
        "# D√©finir les arguments d'entra√Ænement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./longformer_bart_finetuned\",  # R√©pertoire de sortie pour enregistrer les r√©sultats\n",
        "    evaluation_strategy=\"epoch\",  # √âvaluer apr√®s chaque √©poque\n",
        "    learning_rate=2e-5,  # Taux d'apprentissage\n",
        "    per_device_train_batch_size=2,  # Taille du batch pour l'entra√Ænement\n",
        "    per_device_eval_batch_size=2,   # Taille du batch pour l'√©valuation\n",
        "    num_train_epochs=3,  # Nombre d'√©poques d'entra√Ænement\n",
        "    save_steps=500,  # Sauvegarder moins souvent\n",
        "    weight_decay=0.01,  # L2 regularization\n",
        "    logging_dir=\"./logs\",  # R√©pertoire pour les logs\n",
        "    logging_steps=10,  # Fr√©quence des logs\n",
        "    report_to=\"tensorboard\",  # Optionnel : Utiliser TensorBoard pour la visualisation des m√©triques\n",
        ")\n",
        "\n",
        "\n",
        "# Cr√©er un objet Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,  # Le mod√®le combin√© Longformer+BART\n",
        "    args=training_args,  # Les arguments d'entra√Ænement\n",
        "    train_dataset=train_dataset,  # Jeu d'entra√Ænement\n",
        "    eval_dataset=val_dataset,  # Jeu de validation\n",
        "    tokenizer=longformer_tokenizer,  # Tokenizer pour le pr√©-traitement des donn√©es\n",
        ")\n",
        "\n",
        "# Lancer l'entra√Ænement\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T00:30:08.940781Z",
          "iopub.execute_input": "2024-12-28T00:30:08.941535Z",
          "iopub.status.idle": "2024-12-28T00:40:27.780301Z",
          "shell.execute_reply.started": "2024-12-28T00:30:08.941501Z",
          "shell.execute_reply": "2024-12-28T00:40:27.779440Z"
        },
        "id": "ErHnPpsUla0v",
        "outputId": "85f0a7d8-c8aa-4d7c-aee2-b9ea5f69abcc",
        "colab": {
          "referenced_widgets": [
            "15cba45261994c11a1dca8e0fc38f3be",
            "541843278cef48cea7a00acd750265d8",
            "075a8d822f4b4ea2b4f8def9d234dcd3",
            "df5b6298b21946f596f624ea379c043c",
            "4733cc57de6a40459792bf4ddc1304ad",
            "4b156fdcb2f44a9482e945ec07188582",
            "c8d97cf35a504803bed1c85c49abf486"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15cba45261994c11a1dca8e0fc38f3be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "541843278cef48cea7a00acd750265d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "075a8d822f4b4ea2b4f8def9d234dcd3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df5b6298b21946f596f624ea379c043c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4733cc57de6a40459792bf4ddc1304ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b156fdcb2f44a9482e945ec07188582"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/402 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8d97cf35a504803bed1c85c49abf486"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Train dataset size: 321\nVal dataset size: 81\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='483' max='483' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [483/483 09:56, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.052100</td>\n      <td>0.226625</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.032800</td>\n      <td>6.360270</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.029600</td>\n      <td>7.589591</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=483, training_loss=0.5209710441884541, metrics={'train_runtime': 598.0181, 'train_samples_per_second': 1.61, 'train_steps_per_second': 0.808, 'total_flos': 0.0, 'train_loss': 0.5209710441884541, 'epoch': 3.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Dupliquer les poids partag√©s pour √©viter la duplication de m√©moire\n",
        "# Cela doit √™tre fait apr√®s l'entra√Ænement et avant la sauvegarde\n",
        "\n",
        "import safetensors.torch as st\n",
        "\n",
        "# Pour Longformer - dupliquer les embeddings\n",
        "with torch.no_grad():  # Emp√™che la modification des gradients\n",
        "    new_word_embeddings = model.longformer.embeddings.word_embeddings.weight.clone()\n",
        "    model.longformer.embeddings.word_embeddings = torch.nn.Embedding.from_pretrained(new_word_embeddings)\n",
        "\n",
        "# Pour BART - dupliquer les embeddings de l'encodeur et du d√©codeur\n",
        "with torch.no_grad():  # Emp√™che la modification des gradients\n",
        "    new_shared_weight = model.bart.model.shared.weight.clone()\n",
        "    model.bart.model.shared = torch.nn.Embedding.from_pretrained(new_shared_weight)\n",
        "\n",
        "    new_encoder_embed = model.bart.model.encoder.embed_tokens.weight.clone()\n",
        "    model.bart.model.encoder.embed_tokens = torch.nn.Embedding.from_pretrained(new_encoder_embed)\n",
        "\n",
        "    new_decoder_embed = model.bart.model.decoder.embed_tokens.weight.clone()\n",
        "    model.bart.model.decoder.embed_tokens = torch.nn.Embedding.from_pretrained(new_decoder_embed)\n",
        "\n",
        "# Sauvegarder les poids dans un fichier classique\n",
        "torch.save(model.state_dict(), 'model_weights.pth')\n",
        "\n",
        "# Charger manuellement avec safetensors (si vous avez configur√© safetensors)\n",
        "import safetensors.torch as st\n",
        "\n",
        "# Sauvegarder avec safetensors apr√®s avoir dupliqu√© les poids\n",
        "st.save_file(model.state_dict(), 'model_weights.safetensors')\n",
        "\n",
        "# Sauvegarder le mod√®le fine-tun√©\n",
        "trainer.save_model(\"./longformer_bart_finetuned\")\n",
        "\"\"\"\n",
        "# Sauvegarde du mod√®le\n",
        "torch.save(longformer_bart_model.state_dict(), \"longformer_bart_finetuned.pth\")\n",
        "\n",
        "# Sauvegarde de la configuration\n",
        "longformer_bart_model.config.to_json_file(\"longformer_bart_config.json\")\n",
        "\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "id": "ik7qS8SXla0v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sauvegarde de ce mod√®le ainsi cr√©√© et fine-tun√©"
      ],
      "metadata": {
        "id": "pzK8vkd2la0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On zipe le dossier du mod√®le ainsi cr√©√© et entra√Æn√©, pour pouvoir le sauvegarder et le r√©utiliser ensuite sur notre ensemble de test :"
      ],
      "metadata": {
        "id": "GSJNWv9nla0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir(\"./\"))\n",
        "\n",
        "shutil.make_archive('/kaggle/working/longformer_bart_finetuned', 'zip', './longformer_bart_finetuned')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T01:28:29.159189Z",
          "iopub.execute_input": "2024-12-28T01:28:29.159511Z",
          "iopub.status.idle": "2024-12-28T01:42:54.702503Z",
          "shell.execute_reply.started": "2024-12-28T01:28:29.159485Z",
          "shell.execute_reply": "2024-12-28T01:42:54.701773Z"
        },
        "id": "LcQutPSnla0w",
        "outputId": "ad4d47c0-8f53-4a2f-97d8-305b5d114db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "['config.json', 'longformer_bart_finetuned.pth', 'longformer_bart_finetuned', 'longformer_bart_finetuned_config.json', '.virtual_documents', 'longformer_bart_finetuned.zip', 'longformer_bart_finetuned.safetensors', 'logs']\n",
          "output_type": "stream"
        },
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/kaggle/working/longformer_bart_finetuned.zip'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement de notre mod√®le fine-tun√© et constitu√© √† partir des deux mod√®les bart et longformer, pour pouvoir l'utiliser pour l'ensemble de test et soumettre nos r√©sum√©s pour la comp√©tition"
      ],
      "metadata": {
        "id": "4V2jGq8Vla0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Charger le mod√®le fine tun√© afin de l'utiliser\n",
        "import torch\n",
        "from transformers import LongformerModel, BartForConditionalGeneration, LongformerConfig\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# D√©finir la classe LongformerBart\n",
        "class LongformerBart(nn.Module):\n",
        "    def __init__(self, longformer_model, bart_model):\n",
        "        super(LongformerBart, self).__init__()\n",
        "        self.longformer = longformer_model\n",
        "        self.bart = bart_model\n",
        "        self.longformer_projection = nn.Linear(768, 1024)  # Projeter de 768 √† 1024\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids=None, labels=None):\n",
        "        encoder_outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
        "        encoder_hidden_states = self.longformer_projection(encoder_hidden_states)  # Appliquer la projection\n",
        "        decoder_outputs = self.bart(input_ids=decoder_input_ids, labels=labels, encoder_outputs=encoder_hidden_states)\n",
        "        return decoder_outputs\n",
        "\n",
        "# Charger Longformer et BART\n",
        "longformer_model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
        "bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Charger les poids de notre mod√®le fine-tun√©\n",
        "model = LongformerBart(longformer_model, bart)\n",
        "model.load_state_dict(torch.load('/kaggle/input/longformer_bart_finetuned/pytorch/default/1/longformer_bart_finetuned/longformer_bart_finetuned.pth'))  # Remplacez par le bon chemin\n",
        "\n",
        "# Charger la configuration\n",
        "# Notez que vous devez avoir la configuration JSON pour le mod√®le personnalis√©\n",
        "config = LongformerConfig.from_json_file('/kaggle/input/longformer_bart_finetuned/pytorch/default/1/longformer_bart_finetuned/longformer_bart_finetuned_config.json')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T16:14:14.564122Z",
          "iopub.execute_input": "2024-12-28T16:14:14.564479Z",
          "iopub.status.idle": "2024-12-28T16:14:49.546188Z",
          "shell.execute_reply.started": "2024-12-28T16:14:14.564449Z",
          "shell.execute_reply": "2024-12-28T16:14:49.545539Z"
        },
        "id": "DwIbM6Adla0x",
        "outputId": "12f7616a-9c2d-451a-c888-ca8054b1e5db",
        "colab": {
          "referenced_widgets": [
            "e303a2a5c790414695a4a5a2893e2ceb",
            "3e0d48fb6f014f92952d78fd1ae4be51",
            "d33c66df7652461fba3e778263319038",
            "7b5eb43a17484ab6bbb02518a034c0b6",
            "cf1165b305344584bb08f962dc6a64e7"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e303a2a5c790414695a4a5a2893e2ceb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e0d48fb6f014f92952d78fd1ae4be51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d33c66df7652461fba3e778263319038"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b5eb43a17484ab6bbb02518a034c0b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf1165b305344584bb08f962dc6a64e7"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "<ipython-input-5-e8c25c7f1ecc>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/input/longformer_bart_finetuned/pytorch/default/1/longformer_bart_finetuned/longformer_bart_finetuned.pth'))  # Remplacez par le bon chemin\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mise en forme des donn√©es \"OBS\" de test pour le test du mod√®le."
      ],
      "metadata": {
        "id": "XgsQq6hNla0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# D√©finir le chemin du dossier contenant les articles\n",
        "dossier_articles_test = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/test/OBS_test/articles_OBS_test\"\n",
        "\n",
        "# Liste des fichiers dans le dossier\n",
        "fichiers_articles = [f for f in os.listdir(dossier_articles_test) if f.startswith(\"article-\")]\n",
        "\n",
        "# Dictionnaire pour stocker les articles par identifiant (avec contenu des fichiers)\n",
        "articles_test = {}\n",
        "\n",
        "# Remplir le dictionnaire avec les fichiers en fonction des identifiants\n",
        "for fichier in fichiers_articles:\n",
        "    # Extraire l'identifiant du fichier article\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "\n",
        "    # Lire le contenu du fichier\n",
        "    with open(os.path.join(dossier_articles_test, fichier), 'r') as f:\n",
        "        contenu_article = f.read()\n",
        "\n",
        "    # Ajouter l'article et son identifiant au dictionnaire\n",
        "    articles_test[identifiant] = {\"article\": contenu_article}\n",
        "\n",
        "# Convertir le dictionnaire en DataFrame\n",
        "df_articles_test = pd.DataFrame.from_dict(articles_test, orient='index')\n",
        "\n",
        "# R√©initialiser l'index pour que 'identifiant' devienne une colonne normale\n",
        "df_articles_test.reset_index(inplace=True)\n",
        "\n",
        "# Renommer les colonnes\n",
        "df_articles_test.columns = ['identifiant', 'article']\n",
        "\n",
        "# V√©rifier le DataFrame\n",
        "print(df_articles_test.head())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-28T16:44:09.636698Z",
          "iopub.execute_input": "2024-12-28T16:44:09.637002Z",
          "iopub.status.idle": "2024-12-28T16:44:09.675130Z",
          "shell.execute_reply.started": "2024-12-28T16:44:09.636978Z",
          "shell.execute_reply": "2024-12-28T16:44:09.674312Z"
        },
        "id": "ODhgpuc_la0y",
        "outputId": "8ff7b7f8-d25c-49a6-a8d8-1e24f5b1fd32"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "  identifiant                                            article\n0    28278130  Cardiometabolic Risk Factors Among 1.3 Million...\n1    34555924  Prostate Cancer Screening and Incidence among ...\n2    35157313  The associated burden of mental health conditi...\n3    36906849  Implementing digital systems to facilitate gen...\n4    37226713  Patient-reported treatment response in chronic...\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisation des donn√©es et g√©n√©ration des r√©sum√©s pour les articles de test de type \"OBS\""
      ],
      "metadata": {
        "id": "N9AuRBI4la0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerTokenizer, BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Initialiser les tokenizers Longformer et BART\n",
        "longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Charger un mod√®le BART pour la g√©n√©ration de r√©sum√©\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenisation des articles et des r√©sum√©s avec Longformer et BART respectivement.\n",
        "    \"\"\"\n",
        "    # Si 'examples' est une cha√Æne de texte, placez-la sous une cl√© dans un dictionnaire\n",
        "    if isinstance(examples, str):\n",
        "        examples = {\"article\": examples}\n",
        "\n",
        "    # Tokenisation de l'article avec Longformer\n",
        "    model_inputs = longformer_tokenizer(\n",
        "        examples[\"article\"],\n",
        "        max_length=1024,  # Longformer est capable de g√©rer de longs textes\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # Assurer le padding\n",
        "        return_tensors=\"pt\"  # Retourner des tensors PyTorch\n",
        "    )\n",
        "\n",
        "    # Tokenisation des r√©sum√©s avec BART\n",
        "    labels = bart_tokenizer(\n",
        "        examples[\"abstract\"],  # Assume que \"abstract\" est la cible √† r√©sumer\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # Assurer le padding\n",
        "        return_tensors=\"pt\"  # Retourner des tensors PyTorch\n",
        "    )\n",
        "\n",
        "    # Retourne les entr√©es et labels sous forme de dictionnaire\n",
        "    return {**model_inputs, \"labels\": labels[\"input_ids\"]}\n",
        "\n",
        "def generer_resume(article, model, tokenizer, max_length=512):\n",
        "    \"\"\"\n",
        "    Fonction pour g√©n√©rer un r√©sum√© √† partir d'un article scientifique donn√© avec tokenisation.\n",
        "    \"\"\"\n",
        "    # Tokeniser l'article : d√©coupe le texte en tokens et convertit en ids\n",
        "    inputs = tokenizer(\"summarize: \" + article, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
        "\n",
        "    # G√©n√©rer le r√©sum√©\n",
        "    with torch.no_grad():\n",
        "        summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # D√©coder les IDs g√©n√©r√©s pour obtenir le r√©sum√© en texte\n",
        "    resume = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return resume\n",
        "\n",
        "def traiter_ensemble_test(dataset_RCT, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Fonction qui traite l'ensemble du dataset_RCT et g√©n√®re des r√©sum√©s pour chaque article.\n",
        "    \"\"\"\n",
        "    # Convertir le dataset en DataFrame si ce n'est pas d√©j√† fait\n",
        "    df = pd.DataFrame(dataset_RCT)\n",
        "\n",
        "    # Cr√©er une nouvelle colonne pour les r√©sum√©s g√©n√©r√©s\n",
        "    df['resume'] = df['article'].apply(lambda x: generer_resume(x, model, tokenizer))\n",
        "\n",
        "    # Afficher le r√©sultat ou sauvegarder les r√©sultats dans un fichier\n",
        "    return df[['identifiant', 'article', 'resume']]\n",
        "\n",
        "\n",
        "# Traiter l'ensemble de test et g√©n√©rer les r√©sum√©s\n",
        "resultats = traiter_ensemble_test(df_articles_test, model, bart_tokenizer)\n",
        "\n",
        "# Afficher les r√©sultats\n",
        "print(resultats)\n",
        "\n",
        "# Optionnel : Sauvegarder les r√©sultats dans un fichier CSV\n",
        "# resultats.to_csv(\"resultats_resumes.csv\", index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ft0K_TwHla0y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Essai du mod√®le Bart-large-cnn seul"
      ],
      "metadata": {
        "id": "V3SqhVqDla00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BART est un mod√®le de transformer encodeur-d√©codeur (seq2seq : \"sequence to s√©quence\") avec un encodeur bidirectionnel (similaire √† BERT) et un d√©codeur autoregressif (similaire √† GPT).\n",
        "Il est pr√©-entra√Æn√© en masquant des parties du texte et en apprenant √† pr√©dire ces parties - comme si on lui demandait de d√©bruiter un texte bruit√©.\n",
        "Il utilise une attention dense classique pour un transformer, o√π chaque token de l'entr√©e prend en compte tous les autres tokens de la s√©quence. Cela le handicape donc pour nos articles longs (comme tous les mod√®les √† attention dense classique)."
      ],
      "metadata": {
        "id": "_ogvtW3usO7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A vous de le mettre ici, de l'entrainer et de le tester ensuite."
      ],
      "metadata": {
        "id": "_Jxq1tZ3la00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Les trucs ci-dessous sont des restes du chantier, qui ne seront probablement pas utiles en fait (je les garde pour l'instant, \"au cas o√π\")."
      ],
      "metadata": {
        "id": "J_h5Ti2mla00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parce qu'en fait on est sens√©s soumettre nos r√©sum√©s g√©n√©r√©s sur l'ensemble de test dans la comp√©tition (on a droit √† plusieurs soumissions), et c'est la comp√©tition qui va nous renvoyer le score rouge de nos r√©sum√©s (je n'ai pas encore fait et je ne sais donc pas comment √ßa marche pour l'instant - je vais bient√¥t essayer)."
      ],
      "metadata": {
        "id": "n5A5kbqTla00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "f4Y4c_G2la01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install rouge_score\n",
        "from transformers import LongformerTokenizer, BartTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "import os\n",
        "\n",
        "# Initialiser les tokenizers Longformer et BART\n",
        "Longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Si 'examples' est une cha√Æne de texte, placez-la sous une cl√© dans un dictionnaire\n",
        "    if isinstance(examples, str):\n",
        "        examples = {\"article\": examples}\n",
        "\n",
        "    # Tokeniser les articles avec Longformer\n",
        "    model_inputs = Longformer_tokenizer(\n",
        "        examples[\"article\"],\n",
        "        #max_length=4096, #trop long pour le GPU de kaggle...\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # Assurer le padding\n",
        "        return_tensors=\"pt\"  # Retourner des tensors PyTorch\n",
        "    )\n",
        "    labels = bart_tokenizer(\n",
        "        #examples[\"highlights\"],\n",
        "        examples[\"abstract\"],\n",
        "        #max_length=200,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # Assurer le padding\n",
        "        return_tensors=\"pt\"  # Retourner des tensors PyTorch\n",
        "    )\n",
        " # Ajouter les labels au dictionnaire de sortie\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "# Chemin vers les articles\n",
        "#articles_path = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/test/OBS_test/articles_OBS_test\"\n",
        "\n",
        "\n",
        "#######################################\n",
        "\n",
        "# D√©finir les chemins vers les r√©pertoires des articles et des r√©sum√©s\n",
        "articles_path = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/RCT/articles_RCT\"\n",
        "reference_summaries_path = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/RCT/abstracts_RCT\"\n",
        "\n",
        "\n",
        "\n",
        "###########################################\n",
        "# Initialisation de ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n",
        "\n",
        "# Stocker les scores\n",
        "scores = []\n",
        "\n",
        "# R√©sumer les 5 premiers fichiers texte\n",
        "for idx, file_name in enumerate(os.listdir(articles_path)):\n",
        "    if idx >= 5:  # Limiter √† 5 fichiers\n",
        "        break\n",
        "\n",
        "    file_path = os.path.join(articles_path, file_name)\n",
        "    try:\n",
        "        # Lire le contenu avec encodage explicite\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Erreur d'encodage pour le fichier : {file_name}. Tentative avec 'latin-1'.\")\n",
        "        with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
        "            content = f.read()\n",
        "\n",
        "    # R√©sum√© g√©n√©r√© par le mod√®le\n",
        "    inputs = tokenize_function(content)  # Limiter √† 1024 tokens\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=200, num_beams=5, early_stopping=True)\n",
        "    generated_summary = tokenize_function.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # R√©sum√© de r√©f√©rence (remplacement de 'article-' par 'abstract-')\n",
        "    reference_file_name = file_name.replace(\"article-\", \"abstract-\")\n",
        "    reference_file_path = os.path.join(reference_summaries_path, reference_file_name)\n",
        "\n",
        "    # V√©rifier si le fichier de r√©f√©rence existe\n",
        "    if not os.path.exists(reference_file_path):\n",
        "        print(f\"Fichier de r√©f√©rence manquant pour : {file_name}.\")\n",
        "        continue\n",
        "\n",
        "    with open(reference_file_path, \"r\", encoding=\"utf-8\") as ref_file:\n",
        "        reference_summary = ref_file.read()\n",
        "\n",
        "    # Calcul de ROUGE-2\n",
        "    rouge_score = scorer.score(reference_summary, generated_summary)\n",
        "    scores.append(rouge_score['rouge2'].fmeasure)  # R√©cup√©rer le F1-score\n",
        "\n",
        "    print(f\"R√©sum√© pour {file_name} :\")\n",
        "    print(f\"R√©sum√© g√©n√©r√© : {generated_summary}\")\n",
        "    print(f\"Score ROUGE-2 : {rouge_score['rouge2'].fmeasure:.4f}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Calcul de la moyenne des scores\n",
        "if scores:\n",
        "    average_rouge2 = sum(scores) / len(scores)\n",
        "    print(f\"Score moyen ROUGE-2 sur les articles : {average_rouge2:.4f}\")\n",
        "else:\n",
        "    print(\"Aucun score ROUGE-2 calcul√©.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "22ODq3K2la01",
        "outputId": "70bf33c5-a471-47b9-c49b-c1c867893ac6"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-cc9766b53f84>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Lire le contenu de l'article\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0marticle_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Lire le contenu de l'abstract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x96 in position 1736: invalid start byte"
          ],
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0x96 in position 1736: invalid start byte",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#A faire"
      ],
      "metadata": {
        "id": "71BHna2eNf7X",
        "trusted": true,
        "execution": {
          "execution_failed": "2024-12-27T00:34:33.967Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi #afficher des informations sur les GPU NVIDIA disponibles sur la machine"
      ],
      "metadata": {
        "id": "aVhffGQjyN4P",
        "trusted": true,
        "execution": {
          "execution_failed": "2024-12-27T00:34:33.967Z"
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}