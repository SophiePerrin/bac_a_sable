{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 90248,
          "databundleVersionId": 10480827,
          "sourceType": "competition"
        },
        {
          "sourceId": 10266055,
          "sourceType": "datasetVersion",
          "datasetId": 6350823
        }
      ],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SophiePerrin/bac_a_sable/blob/main/notebook_de_marius.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "5YxmUBWFPuiO"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "m_2_maliash_resume_darticles_scientifiques_path = kagglehub.competition_download('m-2-maliash-resume-darticles-scientifiques')\n",
        "komboujeanmarius_bigbirds_pegasus_large_arxiv_path = kagglehub.dataset_download('komboujeanmarius/bigbirds-pegasus-large-arxiv')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Ot5YUlA7PuiX"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-25T05:54:03.071011Z",
          "iopub.execute_input": "2024-12-25T05:54:03.071286Z",
          "iopub.status.idle": "2024-12-25T05:54:03.084572Z",
          "shell.execute_reply.started": "2024-12-25T05:54:03.071263Z",
          "shell.execute_reply": "2024-12-25T05:54:03.083784Z"
        },
        "id": "JN0pB4AmPuiY",
        "outputId": "1ec2fab8-eb25-4fad-d198-7c2d4c83a61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Efficacy and Safety of PF-07038124 in Patients With Atopic Dermatitis and Plaque Psoriasis: A Randomized Clinical Trial\n\nIntroduction\nAtopic dermatitis (AD) and plaque psoriasis are chronic, inflammatory skin diseases associated with a substantial health-related and socioeconomic burden.1,2 Topical treatments, such as daily emollients and corticosteroids, are first-line therapies for mild to moderate AD and plaque psoriasis3,4; however, corticosteroids are associated with systemic adverse events\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pourquoi avoir un appa\n",
        "\n",
        "raiement ? C'est quoi le but ?"
      ],
      "metadata": {
        "id": "F39BqkRMPuib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "trusted": true,
        "id": "uvHwsTL8Puif"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Lire un fichier texte comme exemple\n",
        "file_path = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/test/RCT_test/articles_RCT_test/article-38117526.txt\"\n",
        "\n",
        "# Afficher le contenu du fichier\n",
        "with open(file_path, \"r\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "print(content[:500])  # Afficher les 500 premiers caractères pour vérifier le contenu"
      ],
      "metadata": {
        "trusted": true,
        "id": "f_htPmZKPuih"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/submission.csv\"\n",
        "\n",
        "# Charger le CSV dans un DataFrame\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Afficher les premières lignes\n",
        "print(df.head())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-24T08:49:09.200615Z",
          "iopub.execute_input": "2024-12-24T08:49:09.200928Z",
          "iopub.status.idle": "2024-12-24T08:49:09.22503Z",
          "shell.execute_reply.started": "2024-12-24T08:49:09.200906Z",
          "shell.execute_reply": "2024-12-24T08:49:09.22423Z"
        },
        "id": "TimNM3EOPuii",
        "outputId": "67f57259-997e-4fea-f47b-1ce270d8c651"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "         id                                           abstract\n0  38627831  Goodbye Hartmann trial: a prospective, interna...\n1  38350309  Cumulative oxytocin dose in spontaneous labour...\n2  38649906  Raspberry leaf (Rubus idaeus) use in pregnancy...\n3  38536065  Travel Distance Between Participants in US Tel...\n4  37586661  Effectiveness of intravenous immunoglobulin th...\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Lister uniquement les fichiers et répertoires au premier niveau\n",
        "input_path = \"/kaggle/input\"\n",
        "for entry in os.listdir(input_path):\n",
        "    entry_path = os.path.join(input_path, entry)\n",
        "    if os.path.isdir(entry_path):\n",
        "        print(f\"Répertoire : {entry_path}\")\n",
        "    else:\n",
        "        print(f\"Fichier : {entry_path}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-24T08:49:20.67807Z",
          "iopub.execute_input": "2024-12-24T08:49:20.678339Z",
          "iopub.status.idle": "2024-12-24T08:49:20.683757Z",
          "shell.execute_reply.started": "2024-12-24T08:49:20.678319Z",
          "shell.execute_reply": "2024-12-24T08:49:20.683071Z"
        },
        "id": "Cf5WqCJqPuik",
        "outputId": "9b790d9b-5490-43e7-853d-bec121e5e2aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Répertoire : /kaggle/input/bigbirds-pegasus-large-arxiv\nRépertoire : /kaggle/input/m-2-maliash-resume-darticles-scientifiques\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_path = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/\"\n",
        "print(\"Fichiers dans le répertoire du modèle :\")\n",
        "for file in os.listdir(model_path):\n",
        "    print(file)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-25T06:56:18.507957Z",
          "iopub.execute_input": "2024-12-25T06:56:18.508192Z",
          "iopub.status.idle": "2024-12-25T06:56:18.516886Z",
          "shell.execute_reply.started": "2024-12-25T06:56:18.50817Z",
          "shell.execute_reply": "2024-12-25T06:56:18.516015Z"
        },
        "id": "csi-wUKuPuil",
        "outputId": "37dabac0-1dd4-46ec-e633-a4194a1d3f87"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Fichiers dans le répertoire du modèle :\narticles_OBS\nabstracts_OBS\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "try:\n",
        "    response = requests.get(\"https://www.google.com\", timeout=5)\n",
        "    if response.status_code == 200:\n",
        "        print(\"Connexion internet disponible.\")\n",
        "    else:\n",
        "        print(\"Connexion internet indisponible.\")\n",
        "except requests.ConnectionError:\n",
        "    print(\"Pas de connexion internet.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-24T08:48:45.26668Z",
          "iopub.execute_input": "2024-12-24T08:48:45.266968Z",
          "iopub.status.idle": "2024-12-24T08:48:46.126572Z",
          "shell.execute_reply.started": "2024-12-24T08:48:45.266944Z",
          "shell.execute_reply": "2024-12-24T08:48:46.125663Z"
        },
        "id": "6ygmmtNNPuim",
        "outputId": "bdbacd06-2624-441d-d1ab-e9533f8e9ded"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Connexion internet disponible.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "train_path = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS\"\n",
        "for root, dirs, files in os.walk(train_path):\n",
        "    print(f\"Répertoire : {root}\")\n",
        "    print(f\"Fichiers : {files}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-25T06:18:11.906472Z",
          "iopub.execute_input": "2024-12-25T06:18:11.906772Z",
          "iopub.status.idle": "2024-12-25T06:18:11.910137Z",
          "shell.execute_reply.started": "2024-12-25T06:18:11.906749Z",
          "shell.execute_reply": "2024-12-25T06:18:11.90937Z"
        },
        "id": "IuVHNdudPuip"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verification des fichiers important  pour le modèle \"bigbirds pegasus large\""
      ],
      "metadata": {
        "id": "MRVbifoEPuiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Vérifier le chemin monté pour le dataset\n",
        "dataset_path = \"/kaggle/input/bigbirds-pegasus-large-arxiv\"  # Chemin attendu pour le dataset\n",
        "for file in os.listdir(dataset_path):\n",
        "    print(file)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-25T06:18:30.128683Z",
          "iopub.execute_input": "2024-12-25T06:18:30.128983Z",
          "iopub.status.idle": "2024-12-25T06:18:30.135347Z",
          "shell.execute_reply.started": "2024-12-25T06:18:30.128959Z",
          "shell.execute_reply": "2024-12-25T06:18:30.134583Z"
        },
        "id": "IdtXqCk5Puir",
        "outputId": "599731f0-210b-4b84-cd08-a460db0120bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "config.json\nspiece.model\ntokenizer.json\ntokenizer_config.json\npytorch_model.bin\nspecial_tokens_map.json\ngeneration_config.json\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHARGEMENT DU MODELE BIGBIRDS PEGASUS\n",
        "\n"
      ],
      "metadata": {
        "id": "bQ7bZveZPuis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Chemin vers le modèle local\n",
        "model_path = \"/kaggle/input/bigbirds-pegasus-large-arxiv\"  # Remplacez par le chemin du dataset\n",
        "\n",
        "# Charger le modèle et le tokenizer depuis le répertoire local\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-25T06:57:10.494045Z",
          "iopub.execute_input": "2024-12-25T06:57:10.49435Z",
          "iopub.status.idle": "2024-12-25T06:57:28.326949Z",
          "shell.execute_reply.started": "2024-12-25T06:57:10.494328Z",
          "shell.execute_reply": "2024-12-25T06:57:28.326012Z"
        },
        "id": "le5y-jb5Puis",
        "outputId": "c7b24d96-958f-49fb-9b59-3918cff7a379"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Chemin vers les articles\n",
        "articles_path = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/articles_OBS\"\n",
        "\n",
        "# Résumer chaque fichier texte\n",
        "for file_name in os.listdir(articles_path):\n",
        "    file_path = os.path.join(articles_path, file_name)\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:  # Changer l'encodage si nécessaire\n",
        "            content = f.read()\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Erreur de décodage pour le fichier : {file_name}. Tentative avec 'latin-1'\")\n",
        "        with open(file_path, \"r\", encoding=\"latin-1\") as f:  # Alternative si utf-8 échoue\n",
        "            content = f.read()\n",
        "\n",
        "    # Résumer le contenu\n",
        "    inputs = tokenizer(content[:1024], return_tensors=\"pt\", truncation=True)  # Limiter à 1024 tokens\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=200, num_beams=5, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    #print(f\"Résumé pour {file_name} :\")\n",
        "    #print(summary)\n",
        "    print(\"-\" * 80)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-25T06:57:58.088159Z",
          "iopub.execute_input": "2024-12-25T06:57:58.088646Z",
          "iopub.status.idle": "2024-12-25T07:03:05.912846Z",
          "shell.execute_reply.started": "2024-12-25T06:57:58.088615Z",
          "shell.execute_reply": "2024-12-25T07:03:05.91159Z"
        },
        "id": "NoRoLmdJPuit",
        "outputId": "9ca91fe4-9779-49f7-cada-fa01a3d17a96"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Attention type 'block_sparse' is not possible if sequence_length: 193 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nErreur de décodage pour le fichier : article-37812229.txt. Tentative avec 'latin-1'\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-27e2a749e144>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Résumer le contenu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Limiter à 1024 tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0msummary_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m             \u001b[0;31m# 14. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2064\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3292\u001b[0;31m                 next_token_scores, next_tokens = torch.topk(\n\u001b[0m\u001b[1;32m   3293\u001b[0m                     \u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_tokens_to_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlargest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code pour les 30 premiers résumés de texte"
      ],
      "metadata": {
        "id": "pvUOkuxVPuit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "summaries = []\n",
        "\n",
        "# Parcourir les fichiers\n",
        "for idx, file_name in enumerate(os.listdir(articles_path)):\n",
        "    if idx >= 5:  # Limiter à 5 fichiers\n",
        "        break\n",
        "\n",
        "    file_path = os.path.join(articles_path, file_name)\n",
        "    try:\n",
        "        # Tenter de lire le fichier avec utf-8\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Erreur de décodage pour le fichier : {file_name}. Tentative avec 'latin-1'.\")\n",
        "        # Si utf-8 échoue, essayer avec latin-1\n",
        "        with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
        "            content = f.read()\n",
        "\n",
        "    # Générer le résumé\n",
        "    inputs = tokenizer(content[:1024], return_tensors=\"pt\", truncation=True)\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=200, num_beams=5, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    summaries.append({\"file_name\": file_name, \"summary\": summary})\n",
        "\n",
        "# Sauvegarder dans un fichier CSV\n",
        "df = pd.DataFrame(summaries)\n",
        "df.to_csv(\"summaries.csv\", index=False)\n",
        "print(\"Les résumés ont été sauvegardés dans summaries.csv\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-25T07:07:15.475407Z",
          "iopub.execute_input": "2024-12-25T07:07:15.475818Z",
          "iopub.status.idle": "2024-12-25T07:10:21.902549Z",
          "shell.execute_reply.started": "2024-12-25T07:07:15.475781Z",
          "shell.execute_reply": "2024-12-25T07:10:21.901588Z"
        },
        "id": "GrR2IoSlPuit",
        "outputId": "68e12653-1d92-4c65-83db-abb37375d7a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Erreur de décodage pour le fichier : article-37812229.txt. Tentative avec 'latin-1'.\nLes résumés ont été sauvegardés dans summaries.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from rouge_score import rouge_scorer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Charger le modèle et le tokenizer\n",
        "model_path = \"/kaggle/input/bigbirds-pegasus-large-arxiv\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "\n",
        "# Chemin vers les articles\n",
        "articles_path = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/articles_OBS\"\n",
        "\n",
        "# Chemin des résumés de référence\n",
        "reference_summaries_path = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/abstracts_OBS\"\n",
        "\n",
        "# Initialisation de ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n",
        "\n",
        "# Stocker les scores\n",
        "scores = []\n",
        "\n",
        "# Résumer les 5 premiers fichiers texte\n",
        "for idx, file_name in enumerate(os.listdir(articles_path)):\n",
        "    if idx >= 5:  # Limiter à 5 fichiers\n",
        "        break\n",
        "\n",
        "    file_path = os.path.join(articles_path, file_name)\n",
        "    try:\n",
        "        # Lire le contenu avec encodage explicite\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Erreur d'encodage pour le fichier : {file_name}. Tentative avec 'latin-1'.\")\n",
        "        with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
        "            content = f.read()\n",
        "\n",
        "    # Résumé généré par le modèle\n",
        "    inputs = tokenizer(content[:1024], return_tensors=\"pt\", truncation=True)  # Limiter à 1024 tokens\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=200, num_beams=5, early_stopping=True)\n",
        "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # Résumé de référence (remplacement de 'article-' par 'abstract-')\n",
        "    reference_file_name = file_name.replace(\"article-\", \"abstract-\")\n",
        "    reference_file_path = os.path.join(reference_summaries_path, reference_file_name)\n",
        "\n",
        "    # Vérifier si le fichier de référence existe\n",
        "    if not os.path.exists(reference_file_path):\n",
        "        print(f\"Fichier de référence manquant pour : {file_name}.\")\n",
        "        continue\n",
        "\n",
        "    with open(reference_file_path, \"r\", encoding=\"utf-8\") as ref_file:\n",
        "        reference_summary = ref_file.read()\n",
        "\n",
        "    # Calcul de ROUGE-2\n",
        "    rouge_score = scorer.score(reference_summary, generated_summary)\n",
        "    scores.append(rouge_score['rouge2'].fmeasure)  # Récupérer le F1-score\n",
        "\n",
        "    print(f\"Résumé pour {file_name} :\")\n",
        "    print(f\"Résumé généré : {generated_summary}\")\n",
        "    print(f\"Score ROUGE-2 : {rouge_score['rouge2'].fmeasure:.4f}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Calcul de la moyenne des scores\n",
        "if scores:\n",
        "    average_rouge2 = sum(scores) / len(scores)\n",
        "    print(f\"Score moyen ROUGE-2 sur les articles : {average_rouge2:.4f}\")\n",
        "else:\n",
        "    print(\"Aucun score ROUGE-2 calculé.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-25T07:44:32.935371Z",
          "iopub.execute_input": "2024-12-25T07:44:32.935732Z",
          "iopub.status.idle": "2024-12-25T07:47:45.869086Z",
          "shell.execute_reply.started": "2024-12-25T07:44:32.935708Z",
          "shell.execute_reply": "2024-12-25T07:47:45.868164Z"
        },
        "id": "Qn9SbrnDPuiu",
        "outputId": "f460075a-c09b-4a13-d3fa-778877d98fc0"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Attention type 'block_sparse' is not possible if sequence_length: 193 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Résumé pour article-36868302.txt :\nRésumé généré : in this study, we report on the first data on respiratory outcomes of newborns in intensive care units ( inocs ) of the national university of uzbekistan ( uzbekistan ).<n> we show that the rate of intensive care unit ( inocs ) mortality during the first few months of life of newborns inocs are significantly higher than that of nonpregnant women.<n> moreover, we show that the rate of inocs mortality during the first few months of life of newborns inocs are significantly higher than that of nonpregnant women.<n> our data indicate that intensive care unit ( inocs ) mortality during the first few months of life of newborns inocs in uzbekistan have unprecedented unprecedented global impact [ ].<n> + + + + + + + + + + the national university of uzbekistan ( uzbekistan ), department of general health\nScore ROUGE-2 : 0.0063\n--------------------------------------------------------------------------------\nRésumé pour article-36380385.txt :\nRésumé généré : vascular calcification is a common pathological phenotype characterised by ectopic hydroxyapatite mineral deposition in vascular wall.<n> the general population aged 4575 years has vascular calcification closely associated with inflammation and hardening of arterial wall [ ].<n> the ageing chronic kidney disease [ ], diabetes [ ] and smoking [ ] can induce vascular compliance decrease, arterial wall thickening and plaque rupture [ ], which may result in series of cardiovascular diseases as well as adverse and cardiovascular events [, 7 ]. in this paper, we show that vascular calcification is a major cause of morbidity and mortality being irreversible nature.\nScore ROUGE-2 : 0.0810\n--------------------------------------------------------------------------------\nErreur d'encodage pour le fichier : article-37812229.txt. Tentative avec 'latin-1'.\nRésumé pour article-37812229.txt :\nRésumé généré : * background : * current use of intensive care units ( gold units ) makes prone patients more awake and more focused attention to patients when treatments fail, when treatments fail to provide more peaceful death or recovery, and more focused attention to patients when symptoms fail. <n> * methods : * the purpose of this study is to assess the appropriateness of a protocol of symptom assessment and assessment ( pote ). <n> * results : * the design of the study is based on the observation that prone patients are more likely to suffer from acute symptoms. <n> * conclusions : * the design of the study is based on the observation that prone patients are more likely to suffer from acute symptoms. <n> * methods : * the design of the study is based on the observation that prone patients are more likely to suffer from acute symptoms. <n> * conclusions : * the design of the study is based on the observation that prone patients are more\nScore ROUGE-2 : 0.0509\n--------------------------------------------------------------------------------\nRésumé pour article-35639008.txt :\nRésumé généré : the number of visits to the emergency department increases by more than a factor of two over the age of 10@xmath0@xmath1 years.<n> this rate of increase is much faster than the rate of increase of the number of other visits to the emergency department.<n> this rate of increase is much faster than the rate of increase of the number of other visits to the emergency department. <n> the number of visits to the emergency department increases by more than a factor of two over the age of 10@xmath0@xmath1 years.<n> this rate of increase is much faster than the rate of increase of the number of other visits to the emergency department. <n> the number of visits to the emergency department increases by more than a factor of two over the age of 10@xmath0@xmath1 years.<n> this rate of increase is much faster than the rate of increase of the number of other visits to the emergency department \nScore ROUGE-2 : 0.0162\n--------------------------------------------------------------------------------\nRésumé pour article-37752550.txt :\nRésumé généré : fatigue is one of the most important aspects of health and well - being [ 1,2 ].<n> studies have shown that people with stroke have lower levels of physical activity ( than healthy controls ), which is indirectly associated with fatigue [5,6 ].<n> studies on mild traumatic brain injury have shown that people who are more physically active have lower levels of fatigue [10 ].<n> studies on stroke have shown that people who are more physically active have lower levels of fatigue [11 ].<n> recently, [12 ], [13 ], [14 ] and [15 ] have shown that people with stroke have lower levels of fatigue [16 ], [17 ], [18 ], [19 ], [20 ], [21 ], [21 ], [22 ], [24 ], [24 ]\nScore ROUGE-2 : 0.0546\n--------------------------------------------------------------------------------\nScore moyen ROUGE-2 sur les articles : 0.0418\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}